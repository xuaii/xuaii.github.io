{"posts":[{"title":"[Godot] 缓动曲线节点设计","content":"动机 Tween 节点可以用于创建补间动画，但是由于只能指定固定缓动函数，并且不能使用 value = f(time) 的形式计算值，所以需要设计一个新的缓动函数类，并实现对应的Inspector插件 通常的缓动曲线都是选用 三角函数，或者使用贝塞尔曲线绘制的自定义曲线，为了逼近现实的运动感，这里实现一种自动控制系统里常用的二阶曲线 SecondOrderCurves 设计 首先时二阶系统需要的三个参数，在 Godot 中使用 Vector3 表示， 以及运算过程的中间变量 // 使用插件修复 Godot 不识别自定义类型的 Bug [RegisteredTypeAttribute(nameof(SecondOrderCurves), &quot;&quot;, nameof(Godot.Object))] public class SecondOrderCurves : Godot.Object { [Export] public Vector3 Parameters = Vector3.One; private Vector2 xp; private Vector2 y, yd; private float k1, k2, k3; private float T_crit; // 用于处理单时间片无法完成的模拟 } 二阶系统的参数只有三个变量，但是我们需要一个输入函数 X(y), 和得到一个响应函数Y(y)，其中输入函数是必须的，它决定以某一时刻系统的输入, 而响应函数可以在游戏运行时实时模拟。 // 输入函数由一些点组成，是折线函数 [Export] public Array&lt;Rect2&gt; points = new Array&lt;Rect2&gt;() { new Rect2(new Vector2(0, 300), new Vector2(10, 10)), new Rect2(new Vector2(200, 300), new Vector2(10, 10)), new Rect2(new Vector2(200, 150), new Vector2(10, 10)), new Rect2(new Vector2(400, 150), new Vector2(10, 10)), }; 点集是 `X(t)` 的离散表示，对于连续值得输入，需要在两个点之间进行插值，这里简单的采用线性插值 ```c# public Vector2 Interpolate(float x) { for(int i = 0; i < points.Count - 1; i++) { if(points[i].GetCenter().x <= x && x <= points[i+1].GetCenter().x) { float y1 = points[i].GetCenter().y; float y2 = points[i+1].GetCenter().y; float x1 = points[i].GetCenter().x; float x2 = points[i+1].GetCenter().x; return new Vector2(x, (y1 - y2) / (x1 - x2) * (x - x2) + y2); } } return -1 * Vector2.One; } 对于某时刻 `t` 一个 `X(t)` 对应一个X(t); 为了计算 `Y(t)`，这里采用逐帧计算， ```c# // 输入 X(t), 和 delta， 得到当前t时刻的Y(t) public Vector2 Interpolate(Vector2 x, float delta) { Vector2 xd = (x - xp) / delta; xp = x; int iterations = Mathf.CeilToInt(delta / T_crit); // take extra iterations if delta &gt; T_crit delta = delta / iterations; for(int i = 0; i &lt; iterations; i++) { y = y + delta * yd; yd = yd + delta * (x + k3 * xd - y - k1*yd) / k2; } return y; } 编辑器插件设计 可以参考 Godot.Curve 的实现，但是没有开放 gdscript/c# 接口，只能使用c++拓展，比较繁琐，就自行实现显示和交互逻辑。 编辑器绘图 为了能在编辑器中看到效果，需要在编辑器环境中预览响应函数 Y(t) 的形状。所以使用 point_set 存储曲线值， 表现效果如下图所示： &lt;img src=&quot;https://cdn.staticaly.com/gh/xuaii/xuaii.github.io/1.4/static_easing_curve/y_t_curve_2.png &quot; alt=&quot;&quot; title=&quot;y(t)_2&quot;&gt; // 只有标记为 Tool 的脚本才会在编辑器环境被加载 [Tool] public class CurveCanvas : Control { Array&lt;Vector2&gt; point_set = new Array&lt;Vector2&gt;(); Vector2 point_size = Vector2.One * 10; } 模拟每帧的Y(t) 计算： public void ReDraw(SecondOrderCurves curve = null) { // 缓存 cached if(curve != null) { cachedCurve = curve; } else { return; } point_set.Clear(); Vector2 start = Transform(cachedCurve.StartPoint); Vector2 end = Transform(cachedCurve.EndPoint); if(start.x == -1 || end.x == -1 || cachedCurve == null) { return; } // points.Sort(new PointComp()); // Godot.Collection.Array 不支持排序，需要实现一下 cachedCurve.Init(start); int iter = 200; for(int i = 0; i &lt; iter; i++) { Vector2 x = Transform(cachedCurve.Interpolate(cachedCurve.StartPoint.x + i* (cachedCurve.EndPoint.x - cachedCurve.StartPoint.x) / iter)); Vector2 point = cachedCurve.Interpolate(new Vector2(x.y, x.y), 0.01f); point_set.Add(new Vector2(x.x * RectSize.x, (1 - point.x) * RectSize.y)); } Update(); } 由于 Godot 是以右下为正，所以需要对坐标进行转换 private Vector2 Transform(Vector2 point) { return new Vector2(point.x / RectSize.x, (RectSize.y - point.y) / RectSize.y); } 使用 Godot 的绘图函数 public override void _Draw() { // 具体就不展示了，是一些繁琐的代码 DrawMesh(); DrawPoints(); DrawCurve(); DrawInteractive(); } 编辑器交互 为了能够添加/移动/删除图像上的点， 需要实现以下功能： 单击鼠标左键添加点 长按鼠标左键移动点 单机鼠标右键删除点 public override void _Process(float delta) { cover_index = cachedCurve.FindPoint(GetLocalMousePosition()); SafeDragingArray = new Rect2(GetRect().Position, GetRect().Size - Vector2.One * 10); // 退出拖动的调节 if(Dragging &amp;&amp; IsMouseIn &amp;&amp; 0 &lt;= DraggingIndex&amp;&amp; DraggingIndex &lt; cachedCurve.points.Count &amp;&amp; SafeDragingArray.HasPoint(GetLocalMousePosition())) { (cachedCurve.points[DraggingIndex]) = new Rect2(GetLocalMousePosition(), point_size); } Update(); } 监听鼠标输入，在Godot 中连接 CurveCanvas 的鼠标信号 void _on_CurveCanvas_gui_input(InputEvent @event) { if(!IsMouseIn) { return; } if(@event is InputEventMouseButton mouse) { if(mouse.ButtonIndex == (int)ButtonList.Left) { // 1. 创建节点 if(cover_index == -1 &amp;&amp; !mouse.Pressed &amp;&amp; !Dragging) { cachedCurve.points.Add(new Rect2(GetLocalMousePosition(), point_size)); } // 2. 开始拖动 if(cover_index != -1 &amp;&amp; mouse.Pressed) { Dragging = true; DraggingIndex = cover_index; } // 3. 结束拖动 if(!mouse.Pressed) { if(Dragging) ReDraw(); Dragging = false; DraggingIndex = -1; } ReDraw(); } if(mouse.ButtonIndex == (int)ButtonList.Right) { // 删除当前 cover 的节点 if(cover_index != -1 &amp;&amp; !mouse.Pressed) { cachedCurve.points.RemoveAt(cover_index); } ReDraw(); } } } 编辑器插件（踩坑） Godot 的编辑器插件功能非常多，可以添加Dock，Inspector，主屏幕，等插件。几乎可以实现编辑器阶段的的所有拓展；这里我们主要使用 Inspector 插件来编辑 SecondOrderCurves.parameters. plugin.cs 首先在 plugin.cs 中添加加载/删除 CurveEditorInspector 的代码 tips: 千万不能忘记添加[Tool]，第一次 Build 时会报错，然后再Build一次就Ok了，原因是 C# 是需要编译的而gdscript可以热更新，所以干儿子者不如亲儿子。 #if TOOLS using Godot; using System; [Tool] public class plugin : EditorPlugin { CurveEditorInspector inspector; public override void _EnterTree() { inspector = GD.Load&lt;CSharpScript&gt;(&quot;res://addons/curve_editor/CurveEditorInspector.cs&quot;).New() as CurveEditorInspector; AddInspectorPlugin(inspector); } public override void _ExitTree() { RemoveInspectorPlugin(inspector); } } #endif EditorInspectorPlugin.cs EditorInspectorPlugin 是一个大坑，它有两种主要的运行模式： 修改单属性 修改多属性 起初我不明白 AddPropertyEditor 和 AddPropertyEditorForMultipleProperties 有什么区别，他们似乎具有相似的行为。后来理解了，在 AddPropertyEditorForMultipleProperties 中指定的属性名列表和在AddPropertyEditor 中指定属性名，这样Godot就会知道需要序列化哪些字段，并且在来回切换窗口或者充气Godot后，已经被编辑的属性能够不被重置。 using Godot; using System; #if TOOLS public class CurveEditorInspector : EditorInspectorPlugin { public override bool CanHandle(Godot.Object @object) { if(@object is SecondOrderCurves) { AddPropertyEditorForMultipleProperties(&quot;&quot;, new string [] {&quot;F&quot;, &quot;Z&quot;, &quot;R&quot;}, new CurveEditorProperty()); return true; } return false; } public override bool ParseProperty(Godot.Object @object, int type, string path, int hint, string hintText, int usage) { return false; } } #endif CurveEditorProperty.cs 这里需要指出的是，Godot规定修改属性值不能直接修改 只能通过 EmitChanged(property_name, property value); 来修改，该信号会回调UpdateProperty()方法，这里可以访问到被修改的对象(GetEditedObject())和被修改对象的属名(GetEditedProperty()); 值得注意的是，Godot 会在每一此打开Inspector 时调用UpdateProperty(),这之前会重新创建并且加载Inspector 插件对象，所以需要通过被编辑对象来恢复编辑器预览参数，并且使用IsInit 保证仅调用一次; using Godot; using System; #if TOOLS public class CurveEditorProperty : EditorProperty { Control editor; SecondOrderCurves CacheCurve = null; bool IsInit = false; // 缓存被编辑对象 public override void _Ready() { editor = GD.Load&lt;PackedScene&gt;(&quot;res://addons/curve_editor/CurveEditor.tscn&quot;).Instance&lt;Control&gt;(); AddChild(editor); AddFocusable(editor); SetBottomEditor(editor); editor.Connect(&quot;PorpertyChanged&quot;, this, &quot;OnPorpertyChanged&quot;); editor.Call(&quot;Load&quot;); } void OnPorpertyChanged(Vector3 _parameters) { EmitChanged(&quot;parameters&quot;, _parameters); } public override void UpdateProperty() { if(!IsInit) { editor.Call(&quot;Load&quot;, GetEditedObject().Get(&quot;parameters&quot;)); IsInit = true; } editor.Call(&quot;Refresh&quot;, GetEditedObject()); } } #endif CurveEditor.cs 该脚本用于总结三个HSlider的变化,并且生成PorpertyChanged 事件 using Godot; using System; // 这里的tool不能缺少 [Tool] public class CurveEditor : VBoxContainer { [Signal] public delegate void PorpertyChanged(Vector3 _parameters); // move point Vector3 parameters; public void Load(Vector3 parameters) { (GetNode(&quot;F/FHSlider&quot;) as HSlider).Value = (double)parameters.x; (GetNode(&quot;Z/ZHSlider&quot;) as HSlider).Value = (double)parameters.y; (GetNode(&quot;R/RHSlider&quot;) as HSlider).Value = (double)parameters.z; } public void Refresh(SecondOrderCurves curve) { CurveCanvas canvas = GetNode&lt;CurveCanvas&gt;(&quot;CurveCanvas&quot;); canvas.ReDraw(curve); } public void _on_FHSlider_value_changed(float value) { parameters.x = value; EmitSignal(&quot;PorpertyChanged&quot;, parameters); } public void _on_ZHSlider_value_changed(float value) { parameters.y = value; EmitSignal(&quot;PorpertyChanged&quot;, parameters); } public void _on_RHSlider_value_changed(float value) { parameters.z = value; EmitSignal(&quot;PorpertyChanged&quot;, parameters); } } Show Show Way 最后展示一下运行效果 <img src=\"https://cdn.staticaly.com/gh/xuaii/xuaii.github.io/1.4/static_easing_curve/anima_demo.png\" alt=\"\" title=\"anima demo ","link":"https://xuaii.github.io/post/godot-huan-dong-qu-xian-jie-dian-she-ji/"},{"title":"游戏动画中的缓动函数","content":"情景引入 考虑以下情形，采集用户水平输入 float HorizontalInput 来控制 player 的水平位移 void Update(float delta) { velocity.x = HorizontalInput * WalkSpeed * delta; } 这样写有两个缺点： 当影响水平速度的因素不只是水平输入时，不同因素的叠加不方便。不应该是 = 而应该是 +=; 移动完全与输入一直，看起来很僵硬； 解决方案 所以需要有这么一个函数(缓动函数)，它对输入的响应由可控的延迟，但最终会与输入保持一致 // 添加示例函数1.1 为了使缓动曲线更好的模拟现实世界中由于力产生加速度的运动，这里使用一个二阶系统来描述： y+k1⋅y′+k2⋅y′′=x+k3⋅x′y + k_1\\cdot y^{&#x27;} + k_2\\cdot y^{&#x27;&#x27;} = x + k_3 \\cdot x^{&#x27;} y+k1​⋅y′+k2​⋅y′′=x+k3​⋅x′ 等价的写法： y+k1⋅dydt+k2⋅d2yd2t=x+k3⋅dxdty + k_1\\cdot \\frac{\\mathrm{d} y}{\\mathrm{d} t} + k_2\\cdot \\frac{\\mathrm{d} ^2 y}{\\mathrm{d}^2 t} = x + k_3 \\cdot \\frac{\\mathrm{d} x}{\\mathrm{d} t} y+k1​⋅dtdy​+k2​⋅d2td2y​=x+k3​⋅dtdx​ 通过调整 k1,k2,k3k_1, k_2,k_3k1​,k2​,k3​ 可以改变图像的形状， // 插入示例图像1.2 现在添加三个变量： f=12πk2,θ=k12k2,r=2k3k1f = \\frac{1}{2\\pi \\sqrt{k_2}}, \\quad \\theta = \\frac{k_1}{2\\sqrt{k_2}}, \\quad r = \\frac{2k_3}{k_1} f=2πk2​​1​,θ=2k2​​k1​​,r=k1​2k3​​ 解方程组可得： k1=θπf,k2=1(2πf)2,k3=rθ2πfk_1 = \\frac{\\theta}{\\pi f}, \\quad k_2 = \\frac{1}{(2\\pi f)^2}, \\quad k_3 = \\frac{r \\theta}{2\\pi f} k1​=πfθ​,k2​=(2πf)21​,k3​=2πfrθ​ 原二阶系统的微分方程变为： y+θπf⋅y′+1(2πf)2⋅y′′=x+rθ2πf⋅x′y +\\frac{\\theta}{\\pi f} \\cdot y^{&#x27;} + \\frac{1}{(2\\pi f)^2} \\cdot y^{&#x27;&#x27;} = x + \\frac{r \\theta}{2\\pi f} \\cdot x^{&#x27;} y+πfθ​⋅y′+(2πf)21​⋅y′′=x+2πfrθ​⋅x′ 这里 f,θ,rf, \\theta, rf,θ,r 都具有现实意义了 fff 以 hzhzhz 为单位，代表系统固有频率，它描述系统对输入变化的响应速度 θ\\thetaθ 代表系统的阻尼系数，描述了系统如何最终趋于稳定 rrr 控制系统的初始响应 当 r=0r = 0r=0 时系统需要花费一点时间才能从禁止开始加速 当 r&gt;0r &gt; 0r&gt;0 时系统立刻对变化做出反应 当 r&gt;1r &gt; 1r&gt;1 时系统变化将冲过目标 当 r&lt;0r &lt; 0r&lt;0 时系统会有抬手运动（先反向运动） 一般为机械链接设置 r=2r = 2r=2 现在只剩下最后一个问题了，如何解二阶系统？ 二阶微分方程数值解 1. 半隐式欧拉法 该方法在该问题下与复杂的 Verlet 积分法 有着相同的精度， 首先计算 x 变化率 xn+1′=xn+1−xnTx^{&#x27;}_{n+1} = \\frac{x_{n+1} - x_n}{T} \\\\ xn+1′​=Txn+1​−xn​​ 然后计算 y 的变化 {yn+1=yn+Tyn′yn+1′=yn′+Ty′′\\begin{cases} y_{n + 1} = y_{n} + Ty^{&#x27;}_{n} \\\\ \\quad\\\\ y^{&#x27;}_{n+1} = y^{&#x27;}_{n} + Ty^{&#x27;&#x27;} \\\\ \\end{cases} ⎩⎪⎨⎪⎧​yn+1​=yn​+Tyn′​yn+1′​=yn′​+Ty′′​ 由于: y′′=x+k3x′−y−k1y′k2y^{&#x27;&#x27;} = \\frac{x + k_3 x^{&#x27;} - y - k_1y^{&#x27;}}{k_2} y′′=k2​x+k3​x′−y−k1​y′​ 所以： yn+1′=yn′+T⋅xn+1+k3xn+1′−yn+1−k1yn′k2y^{&#x27;}_{n+1} = y^{&#x27;}_{n} + T\\cdot \\frac{x_{n+1} + k_3 x^{&#x27;}_{n+1} - y_{n+1} - k_1y^{&#x27;}_{n}}{k_2} yn+1′​=yn′​+T⋅k2​xn+1​+k3​xn+1′​−yn+1​−k1​yn′​​ 这存在一个问题，如果频率 fff 远大于帧率，系统将变得不稳定，会产生无穷大的值： 可以简单地设置 f 的取值范围 通过数学方法确保不发生极端情况 为了提供更多的鲁棒性，采用方案二 分析 不稳定产生的原因，该系统的本质是反馈系统，他的迭代输出将被反馈到后续的迭代用于计算，当帧间时间步长和参数相比太大时，随着时间增加，误差将逐渐累积，当超过某个临界值时，误差会开始滚雪球，迅速导致灾难性后果，为了计算该临界值，引入线性代数方法： {yn+1=yn+Tyn′yn+1′=yn′+T⋅xn+1+k3xn+1′−(yn+Tyn′)−k1yn′k2\\begin{cases} y_{n + 1} = y_{n} + Ty^{&#x27;}_{n} \\\\ \\quad \\\\ y^{&#x27;}_{n+1} = y^{&#x27;}_{n} + T\\cdot \\frac{x_{n+1} + k_3 x^{&#x27;}_{n+1} -(y_{n} + Ty^{&#x27;}_{n}) - k_1y^{&#x27;}_{n}}{k_2} \\end{cases} ⎩⎪⎨⎪⎧​yn+1​=yn​+Tyn′​yn+1′​=yn′​+T⋅k2​xn+1​+k3​xn+1′​−(yn​+Tyn′​)−k1​yn′​​​ 展开合并同类项： {yn+1=yn+Tyn′yn+1′=−Tk2yn+k2−T2−Tk1k2yn′+Tk2xn+1+Tk3k2xn+1′\\begin{cases} y_{n + 1} = y_{n} + Ty^{&#x27;}_{n} \\\\ \\quad \\\\ y^{&#x27;}_{n+1} = \\frac{-T}{k_2} y_n + \\frac{k_2 - T^2 - Tk_1}{k_2}y^{&#x27;}_{n} + \\frac{T}{k_2}x_{n+1} + \\frac{Tk_3}{k_2}x^{&#x27;}_{n+1}\\\\ \\end{cases} ⎩⎪⎨⎪⎧​yn+1​=yn​+Tyn′​yn+1′​=k2​−T​yn​+k2​k2​−T2−Tk1​​yn′​+k2​T​xn+1​+k2​Tk3​​xn+1′​​ 矩阵表示如下： [yy′]n+1=[1T−Tk2k2−T2−Tk1k2]⋅[yy′]n+[00Tk2Tk3k2]⋅[xx′]n+1\\begin{bmatrix} y\\\\ y^{&#x27;} \\end{bmatrix}_{n+1} = \\begin{bmatrix} 1 &amp; T\\\\ -\\frac{T}{k_2} &amp; \\frac{k_2 - T^2 - Tk_1}{k_2} \\end{bmatrix} \\cdot \\begin{bmatrix} y\\\\ y^{&#x27;} \\end{bmatrix}_{n} + \\begin{bmatrix} 0 &amp; 0\\\\ \\frac{T}{k_2} &amp; \\frac{Tk_3}{k_2} \\end{bmatrix} \\cdot \\begin{bmatrix} x\\\\ x^{&#x27;} \\end{bmatrix}_{n+1} [yy′​]n+1​=[1−k2​T​​Tk2​k2​−T2−Tk1​​​]⋅[yy′​]n​+[0k2​T​​0k2​Tk3​​​]⋅[xx′​]n+1​ 简写如下 Yn+1=A⋅Yn+B⋅Xn+1Y_{n+1} = A\\cdot Y_n + B\\cdot X_{n+1} Yn+1​=A⋅Yn​+B⋅Xn+1​ 分析： AAA 称为状态转移矩阵，他表示迭代是如何影响状态变量的，直观来说如果 AAA 矩阵不导致状态变量 YYY 的增长，那该反馈是稳定的；考虑AAA 作为值而非变量，利用特征值理论，记 AAA 的特征值λ\\lambdaλ 如果 λi&lt;1\\lambda_i &lt; 1λi​&lt;1, Y 将逐渐减小趋于稳定 如果 λi&gt;1\\lambda_i &gt; 1λi​&gt;1, Y 迅速增大，很快变得无法控制 计算特征值如下： det(A−λI)=0det(A - \\lambda I) = 0 det(A−λI)=0 展开后 k2λ2+(T2+Tk1−2k2)λ(k2−Tk1)=0k_2 \\lambda^2 + (T^2 + Tk_1 - 2k_2)\\lambda (k_2-Tk_1) = 0 k2​λ2+(T2+Tk1​−2k2​)λ(k2​−Tk1​)=0 解关于 λ\\lambdaλ 的二次方程 λ=−b±b2−4ac2a\\lambda = \\frac{ -b \\pm \\sqrt{b^2-4ac}}{2a} λ=2a−b±b2−4ac​​ 令 ∣λ∣&lt;1|\\lambda| &lt; 1∣λ∣&lt;1: 得: T&lt;4k2+k12−k1T &lt; \\sqrt{4k_2 + k_1^2} - k_1 T&lt;4k2​+k12​​−k1​ 这里 如果时间步长大于临界值, 将拆分为多个了迭代来计算 public classs SecondOrderDynamics { private float T_crit; // critical stable time step public SecondOrderDynamics(float f, floatz, float r, Vector x0) { // update compute constants k1 = z / (PI * f); k2 = 1 / ((2 * PI * f) * (2 * PI * f)); k3 = r * z / (2 * PI * f); T_crit = 0.8f * (sqtr(4 * k2 + k1 * k1)); xp = x0; y = x0; yd = 0; } public Vector Update(float delta, Vector x, Vector xd = null) { if(xd == null) { xd = (x - xp) / delta; xp = x; } int interations = (int)Ceil(delta / T_crit); // take extra iterations if delta &gt; T_crit delta = delta / iterations; for(int i = 0; i &lt; iterations; i++) { y = y + delta * yd; yd = yd + delta * (x + k3 * xd - y - k1*yd) / k2; } return y; } } 此外，如果想避免迭代次数过多，可以限制 k2k_2k2​ 的值（减缓运动） k2&gt;T24+Tk12k_2 &gt; \\frac{T^2}{4} + \\frac{Tk_1}{2} k2​&gt;4T2​+2Tk1​​ float k2_stable = Max(k2, 1.1f * (T*T/4 + T*k1/2)); 2.零极点匹配法（高精度）没懂 public classs SecondOrderDynamics { private Vector xp; private Vector y, yd; private float _w, _z, _d, k1, k2, k3; public SecondOrderDynamics(float f, float z, float r, Vetor x0) { _w = 2 * PI * f; _z = z; _d = _w * sqrt(Abs(z*2-1)); k1 = z / (PI * f); k2 = 1 / (_w * _w); k3 = r * z / _w; xp = x0; y = x0; yd = 0; } public Vector Update(float delta, Vector x, Vector xd = null) { if(xd == null) { float k1_stable, k2_stable; if(_w * T &lt; _z) { k1_stable = k1; k2_stable = Max(k2, T * T / 2+ T * k1 / 2, T*k1); } else { float t1 = Exp(-_z * _w * T); float alpha = 2 * t1 * (_z &lt;= 1 ? cos(T * _d) : cosh(T * _d)); float beta = t1 * t1; float t2 = T / (1 + beta -alpha); k1_stable = (1 - beta) * t2; k2_stable = T * t2; } y = y + T * yd; yd = yd + T * (x + k3*xd -y - k1*yd) / k2_stable; return y; } } } 3. 其他数值解方法（略） 参考: t3ssel8r:Giving Personality to Procedural Animations using Math ","link":"https://xuaii.github.io/post/you-xi-dong-hua-zhong-de-huan-dong-han-shu/"},{"title":"对《戴森球计划》的一些杂乱的想法","content":"通宵玩了戴森球计划，只玩了母星球部分（实在不想肝了 个人看法，这类型游戏没什么乐趣，但是通关卡或者达成某些阶段性的目标会产生巨大的成就感，以程序员的视角，整个游戏过程像是在玩一个巨大的计算器，不断的铺路建造的过程又像是在写代码，但它没有代码灵活（大概是既不让使用设计模式，也不让使用封装继承的感觉，就像是在手写汇编码），其中的重复性太高。 各种物资的意义仅仅是数值意义，类似在玩一个Machinations,；通常游戏会将数值隐藏在机制背后，但是基建类游戏无可避免的需要将各种数值作为主要的玩法，这会让玩家感到疲劳和困。 戴森球计划对几千上万个动画目标的处理应该是用了什么魔法，太流畅了，大概是用了GPU计算动画之类的方法，这是值得借鉴的！！ 此外，作为独立游戏，《戴森球计划》流畅，内容多，耐玩，还能打MOD。尽管不能面面俱到，但是依然是国产独立游戏的劳模了！ 鲁迅说过，游戏就是在规则下，不以容易的方式达成目标。面对挑战和克服困难本来就是游戏的一部分，那戴森球计划带来的枯燥和成就感本就是游戏的乐趣一部分！ ","link":"https://xuaii.github.io/post/dui-lesslessdai-sen-qiu-ji-hua-greatergreater-de-yi-xie-za-luan-de-xiang-fa/"},{"title":"CART回归","content":"决策树.html 导入相关库 pandas：决策树的构建涉及到数据集的一些操作，利用pandas的DataFrame数据结构可以很好方便的完成 copy :在递归时浅拷贝会导致问题,使用copy.deepcopy()进行深拷贝 matplot.pyplot:绘制决策树的划分图像 import pandas as pd import copy import matplotlib.pyplot as plt import numpy as np from math import fabs 导入数据 input = [[0.697, 0.460, 1], [0.774, 0.376, 1], [0.634, 0.264, 1], [0.608, 0.318, 1], [0.556, 0.215, 1], [0.403, 0.237, 1], [0.481, 0.149, 1], [0.437, 0.211, 1], [0.666, 0.091, 0], [0.243, 0.267, 0], [0.245, 0.057, 0], [0.343, 0.099, 0], [0.639, 0.161, 0], [0.657, 0.198, 0], [0.360, 0.370, 0], [0.593, 0.042, 0], [0.719, 0.103, 0]] 定义回归树的节点类Node attrList 节点剩下的属性列表 Dataset 节点划分到的数据集 left/right 左右子树 c 叶节点的预测值 description 该节点的描述（可选） attr 该节点划分属性 s 划分属性的值 考虑到使用非二叉树，在每次寻找最优化分的时候算法复杂度太高，所以此时使用二叉树就OK class Node: def __init__(self, description=&quot;&quot;, c = -1, Dataset=pd.DataFrame(), attrList=[]): self.attrList = attrList self.Dataset = Dataset self.left = None self.right = None self.c = c self.attr = &quot;&quot; self.s = -1 self.desciption = description 计算损失 ℓ=∑xi∈R1(j,s)(yi−c1)2+∑xi∈R2(j,s)(yi−c2)2\\ell = \\sum_{x_i \\in R_1(j,s)}(y_i-c_1)^2+\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2 ℓ=xi​∈R1​(j,s)∑​(yi​−c1​)2+xi​∈R2​(j,s)∑​(yi​−c2​)2 def loss(attr, s, data): D1 = data[data[attr] &lt;= s] D1_mean = D1['label'].std() * D1.size D2 = data[data[attr] &gt; s] D2_mean = D2['label'].std() * D2.size return D1_mean + D2_mean 最小化损失 min⁡j,s(min⁡c1∑xi∈R1(j,s)(yi−c1)2+min⁡c2∑xi∈R2(j,s)(yi−c2)2)\\min_{j,s} ( \\min_{c_1} \\sum_{x_i \\in R_1(j,s)}(y_i-c_1)^2 + \\min_{c_2} \\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2 ) j,smin​(c1​min​xi​∈R1​(j,s)∑​(yi​−c1​)2+c2​min​xi​∈R2​(j,s)∑​(yi​−c2​)2) 此处数据集不大 且 为了实现起来简单就是用了遍历所有属性的值找到最小的损失，应该还有一些方法可以优化找到最小值的过程 def findOptDiv(root): losses = [] for attr in root.attrList: for s in root.Dataset[attr]: losses.append((loss(attr, s, root.Dataset), attr, s)) minLoss = min(losses) return minLoss 二叉树的构建 在以下情况返回IF len(attrList) == 0：此时所有属性已经划分完毕， 就以该集合所有样本的label的均值作预测值 Dataset.size == 1：此时该节点的样本仅有一个 就 以该样本的label值做预测值 ELSE 将样本按最优划分划分为两个集合D1，D2，并分别构建subTree def buildTree(root): # if root.Dataset.size() &lt;= 1: # description = &quot;leaf node&quot; # c_p = root.Dataset['label'].mean() # leaf = Node(description=description, c = c_p) # 如果样本集合中只有一个样本那么该节点为叶节点，该叶节点的预测值是该唯一样本的label if root.Dataset.size == 1: root.c = root.Dataset['label'] return # 如果已经将属性分完了，那么该节点为叶节点，剩下的样本集中label的期望为该叶节点的预测值 elif len(root.attrList) == 0: root.description = &quot;leaf node&quot; root.c = root.Dataset['label'].mean() return else: # 找到最优化分 (_, attr, s) = findOptDiv(root) # 将节点的划分属性设为找到的attr root.attr = copy.deepcopy(attr) # 将按属性attr划分该节点值划分值s设为最优的s root.s = copy.deepcopy(s) # 将样本集合按照找到的最优化分划分为D1， D2 D1 = copy.deepcopy(root.Dataset[root.Dataset[attr] &lt;= s]) D2 = copy.deepcopy(root.Dataset[root.Dataset[attr] &gt; s]) # 将划分该节点属性从属性集合中删除 list_notremoved = copy.deepcopy(root.attrList) root.attrList.remove(attr) list_removed = copy.deepcopy(root.attrList) root.attrList = copy.deepcopy(list_notremoved) # 构建左子树和右子树 root.left = Node(Dataset = D1, attrList=copy.deepcopy(list_removed)) root.right = Node(Dataset = D2, attrList=copy.deepcopy(list_removed)) buildTree(root.left) buildTree(root.right) return root 预测函数 def predict(x, root): while(len(root.attrList) != 0): if x[root.attr] &lt; root.s: root = root.left else: root = root.right return root.c 评估函数 def evaluate(data_test, root): accuracy = 0 for i in range(len(data_test)): res = predict(data_test.loc[i], root) # 将回归问题转为分类问题 res = .5 if res &gt; .5 else 0 accuracy += fabs(res - data_test.loc[i][&quot;label&quot;]) return 1 - accuracy / len(data_test) data = pd.DataFrame(input, columns=['密度','含糖率',&quot;label&quot;]) root = Node(Dataset=data, attrList = ['密度','含糖率']) root = buildTree(root) 可以大致看出决策过程 先看含糖率： 小于.13 小于.666 坏瓜 大于.666 好瓜 大于.13 小于.697 0.6的概率是好瓜 大于.697 1的概率是好瓜 print(root.attr,root.s) print(root.left.attr,root.left.s,root.left.left.c,root.left.right.c) print(root.right.attr,root.right.s,root.right.left.c,root.right.right.c) 含糖率 0.103 密度 0.666 0.0 0.0 密度 0.697 0.6363636363636364 1.0 可视化和评估模型表现 s1 = root.s s21 = root.left.s s22 = root.right.s plt.plot([.2,.8],[s1,s1],'r-') plt.plot([s21,s21],[0,s1],'r-') plt.plot([s22,s22],[s1,.5],'r-') for plot in input: if plot[2] == 0: c = 'red' else: c = 'green' plt.scatter(plot[0],plot[1],c=c) print(&quot;THE ACCURACY OF REGRESSION TREE IS {}&quot;.format(evaluate(data, root))) THE ACCURACY OF REGRESSION TREE IS 0.6176470588235294 ","link":"https://xuaii.github.io/post/cart-hui-gui/"},{"title":"线性模型","content":"线性模型.html import numpy as np import matplotlib.pyplot as plt import copy from IPython import display np.random.seed(0) data = [[0.697, 0.460, 1], [0.774, 0.376, 1], [0.634, 0.264, 1], [0.608, 0.318, 1], [0.556, 0.215, 1], [0.403, 0.237, 1], [0.481, 0.149, 1], [0.437, 0.211, 1], [0.666, 0.091, 0], [0.243, 0.267, 0], [0.245, 0.057, 0], [0.343, 0.099, 0], [0.639, 0.161, 0], [0.657, 0.198, 0], [0.360, 0.370, 0], [0.593, 0.042, 0], [0.719, 0.103, 0]] def load_data(): X = np.array(data).T label = copy.deepcopy(X[2,:].reshape(1,17)) X[2,:] = 1 assert X.shape == (3,17) assert label.shape == (1,17) return X, label l(β)=∑i=1m(−yiβTxi+ln(1+eβTxi))l(\\beta) = \\sum_{i=1}^{m}(-y_i \\beta^Tx_i+ln(1+e^{\\beta^T x_i})) l(β)=i=1∑m​(−yi​βTxi​+ln(1+eβTxi​)) =∑i=1m−yiβTxi+∑i=1mln(1+eβTxi)= \\sum_{i=1}^{m}-y_i \\beta^Tx_i+\\sum_{i=1}^{m}ln(1+e^{\\beta^T x_i}) =i=1∑m​−yi​βTxi​+i=1∑m​ln(1+eβTxi​) =(βTX)Y+np.sum(ln(1+eYi))= (\\beta^TX)Y + np.sum(ln(1+e^{Y_i})) =(βTX)Y+np.sum(ln(1+eYi​)) 使用numpy实现如下： part_1 = np.dot(Y, label.T) part_2 = np.sum(np.ln(1+np.exp(Y))) def loss(label, Y): part_1 = -np.dot(Y, label.T) part_2 = np.sum(np.log(1+np.exp(Y))) return part_1 + part_2 前向传播 input=W,Xinput = W,Xinput=W,X output=W′TX′=WTX+boutput = W^{&#x27;T} X^{&#x27;} = W^T X+boutput=W′TX′=WTX+b def forward_propagation(W,X): Z = np.dot(W,X) A = 1/(1+np.exp(-Z)) return A 初始化参数: 使用随机初始化 n ------ 特征值数量为 m ------ 样本数量 w′=(w,b)w^{&#x27;} = (w,b)w′=(w,b) w.shape==（1,n+1）w.shape == （1,n+1）w.shape==（1,n+1） def initialization(n): return np.random.randn(1,n+1) # return np.zeros((1,n+1)) 反向传播 dw=−∑i=1mxi(yi−p1(xi;β))dw = -\\sum_{i=1}^{m}x_i(y_i-p_1(x_i;\\beta)) dw=−i=1∑m​xi​(yi​−p1​(xi​;β)) dw=−(label−Y)XTdw = -(label-Y)X^T dw=−(label−Y)XT def back_propagation(X,Y,label): return -np.dot((label-Y),X.T) 训练循环 使用梯度下降法 学习率 = 0.5 迭代次数 10000 def train(epoch=100,n = 2, learning_rate = 0.5, detial = True): X, label = load_data() W = initialization(n) losses = [] W_list = [] for i in range(0,epoch): Y = forward_propagation(W,X) l = loss(label, Y) dw = back_propagation(X,Y,label) losses.append(float(l)) W += learning_rate * dw if i % 100 == 0: W_list.append(copy.deepcopy(W)) if i % 1000 == 0 and detial ==True: print(&quot;This is {}th epoch , loss = {}&quot;.format(i, l)) return W_list, losses def draw(W): W = W[0] x = np.linspace(0.2,0.8,100) y = -(W[0] * x + W[2])/W[1] plt.plot(x,y) for plot in data: if plot[2] == 0: c = 'red' else: c = 'green' plt.scatter(plot[0],plot[1],c=c) def show(W_list): for W in W_list: plt.clf() display.clear_output(wait=True) draw(W) plt.pause(0.001) W_list,losses = train(epoch=10000,n=2,learning_rate=.01,detial=False) show(W_list[:100]) plt.plot(losses) [&lt;matplotlib.lines.Line2D at 0x7f1b3a14aa50&gt;] ","link":"https://xuaii.github.io/post/xian-xing-mo-xing/"},{"title":"Neural Network","content":"L层神经网络.html Neural Network 实现方式：采用BP算法，使用梯度下降来优化W，b 需要实现的函数如下： initial():根据用户自定义的层layer-&gt;[ n1,n2,n3,...,nmn_1, n_2,n_3,...,n_mn1​,n2​,n3​,...,nm​ ]来初始化参数W，b sigmoid():输入X∈RmxnX \\in R^{m x n}X∈Rmxn ，返回A=11+e−WX+bA = \\frac{1}{1+e^{-WX + b}}A=1+e−WX+b1​ liner_propagation():实现一层的前向传播，返回 A，并且缓存中间量Z L_layers_propagation():实现L层的前向传播，并且缓存所有中间层的A，Z predict():使用训练好的模型Ws，bs预测某一输入特征向量对应的预测值 back_propagation():实现一层的反向传播 L_back_propagation():实现L层链式反向传播 update_parameters():每一个epoch更新参数 evaulate():评估模型 model():主循环，BP算法梯度下降的循环 load_data():加载数据 在整体的设计中并没有loss函数的出现，是因为，在反向传播过过中，dloss/dW的计算并不涉及loss的值，dloss/dW的表达式中仅有y_truth 和 y_pred 链式偏微分 推导（这里是根据西瓜书上原始公式推导，所以是累计神经网络） 由于在布置编程作业之前就自己实现了一下神经网络，所以实现的是任意层数和任意数量神经元的神经网络，所以就改下参数交作业了 由于神经网络层数可能会非常多，所以在反向传播时loss对每一层的W，b求导会重复很多中间步骤 设：βi∈Rm⋅n设：\\beta^i \\in \\R^{m \\cdot n} 设：βi∈Rm⋅n Ai−1∈Rn⋅sA^{i-1} \\in \\R^{n \\cdot s} Ai−1∈Rn⋅s Zi∈Rm⋅sZ^{i} \\in \\R^{m \\cdot s} Zi∈Rm⋅s Zi=WiAi−1+bi=βiAi−1,Z^i = W^i A^{i-1} + b^i = \\beta^i A^{i-1}, Zi=WiAi−1+bi=βiAi−1, Ai=sigmoid(Zi);A^i = sigmoid(Z^i); Ai=sigmoid(Zi); ∂E∂βi=∂Zi∂βi⋅∂Ai∂Zi⋅∂E∂Ai⏞∂Ai⎵∂Zi，（1.1）\\frac{\\partial E}{\\partial \\beta^i} = \\frac{\\partial Z^i}{\\partial \\beta^i} \\cdot \\underbrace{ \\frac{\\partial A^i}{\\partial Z^i} \\cdot \\overbrace {\\frac{\\partial E}{\\partial A^i}}^{\\partial A^i} }_{\\partial Z^i} ， \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad （1.1） ∂βi∂E​=∂βi∂Zi​⋅∂Zi∂Zi∂Ai​⋅∂Ai∂E​​∂Ai​​​，（1.1） ∂E∂Ai−1=∂Zi∂Ai−1⋅∂Ai∂Zi⋅∂E∂Ai⏞∂Ai⎵∂Zi,（1.2）\\frac{\\partial E}{\\partial A^{i-1}} = \\frac{\\partial Z^i}{\\partial A^{i-1}} \\cdot \\underbrace {\\frac{\\partial A^i}{\\partial Z^i} \\cdot \\overbrace{ \\frac{\\partial E}{\\partial A^i} }^{\\partial A^i} }_{\\partial Z^i}, \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad （1.2） ∂Ai−1∂E​=∂Ai−1∂Zi​⋅∂Zi∂Zi∂Ai​⋅∂Ai∂E​​∂Ai​​​,（1.2） 其中： ∂Ai由上一层反向传播提供,\\partial A^i由上一层反向传播提供, ∂Ai由上一层反向传播提供, 现在考虑每一层反向传播的计算 已知: ∂E∂Ak,∂Ak∂Zk,其中：k是层数\\frac{\\partial E}{\\partial A^k}, \\frac{\\partial A^k}{\\partial Z^k}, \\qquad其中：k是层数 ∂Ak∂E​,∂Zk∂Ak​,其中：k是层数 求解: ∂E∂Zki,j其中：i,j∈{i,j∣i∈(1,Zk.shape[0]，j∈(1,Zk.shape[1])}\\frac{\\partial E}{\\partial {Z^k}_{i,j}} \\qquad其中： i,j \\in \\{ i,j | i \\in (1,Z^k.shape[0]， j \\in (1,Z^k.shape[1])\\} ∂Zki,j​∂E​其中：i,j∈{i,j∣i∈(1,Zk.shape[0]，j∈(1,Zk.shape[1])} 分析： 由于 （暂时忽略掉上标，仅在需要的时候添加） Z=βAZ = \\beta A Z=βA Zi,j=∑k=1nβi,kAk,jZ_{i,j} = \\sum_{k=1}^{n}\\beta_{i,k}A_{k,j} Zi,j​=k=1∑n​βi,k​Ak,j​ 该式子表明 Zi,j的值与同行βi,kZ_{i,j}的值与同行\\beta{i,k} Zi,j​的值与同行βi,k 即每一个β与Z的第a行相关即每一个\\beta与Z的第a行相关 即每一个β与Z的第a行相关 那么要计算∂E∂βa,b那么要计算\\frac{\\partial E}{\\partial \\beta_{a,b}} 那么要计算∂βa,b​∂E​ 即需要∂E∂βa,b=∑k=1s∂E∂Za,k∂Za,k∂βa,b即需要\\frac{\\partial E}{\\partial \\beta_{a,b}} = \\sum_{k=1}^{s}\\frac{\\partial E}{\\partial Z_{a,k}} \\frac{\\partial Z_{a,k}}{\\partial \\beta_{a,b}} 即需要∂βa,b​∂E​=k=1∑s​∂Za,k​∂E​∂βa,b​∂Za,k​​ 而：∂Za,k∂βa,b=Ab,k而：\\frac{\\partial Z_{a,k}}{\\partial \\beta_{a,b}} = A_{b,k} 而：∂βa,b​∂Za,k​​=Ab,k​ 那么：∂E∂βa,b=∑k=1s∂E∂Za,k⋅Ab,k那么：\\frac{\\partial E}{\\partial \\beta_{a,b}} = \\sum_{k=1}^{s}\\frac{\\partial E}{\\partial Z_{a,k}} \\cdot A_{b,k} 那么：∂βa,b​∂E​=k=1∑s​∂Za,k​∂E​⋅Ab,k​ ∂E∂βa,b=∑k=1s(∂E∂Z)a,k⋅Ab,k\\frac{\\partial E}{\\partial \\beta_{a,b}} = \\sum_{k=1}^{s} {(\\frac{\\partial E}{\\partial Z})}_{a,k} \\cdot A_{b,k} ∂βa,b​∂E​=k=1∑s​(∂Z∂E​)a,k​⋅Ab,k​ =∑k=1s(∂E∂Z)a,k(AT)k,b= \\sum_{k=1}^{s}(\\frac{\\partial E}{\\partial Z})_{a,k}(A^T)_{k,b} =k=1∑s​(∂Z∂E​)a,k​(AT)k,b​ 所以有：∂Eβ=dZAT所以有：\\frac{\\partial E}{\\beta} = dZ A^T 所以有：β∂E​=dZAT 由于推导的迭代式子中包含A, Z, 等前向传播产生的中间结果， 所以在前向传播时需要将它们缓存下来 import numpy as np import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation import sys import copy from IPython import display import time from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report, roc_auc_score # 设置随机种子，避免每次的结果不一样 np.random.seed(1) %matplotlib inline 初始化函数initial(layers) input: layers -&gt; [layer_1, layer_2, layer_3, ..., layer_m] output: paramters -&gt; dict() parameters[&quot;w&quot;] = temp_w -&gt; list() parameters['layers'] = layers -&gt; list() def initial(layers): parameters = dict() temp_w = list() for i in range(1, len(layers)): temp_w.append(np.random.randn(layers[i], layers[i-1])) parameters[&quot;w&quot;] = temp_w parameters['layers'] = layers return parameters 激活函数 Sigmoid() 激活函数种类： sigmoid():&quot;S&quot;形函数f(x)=11+e−WX+bf(x) = \\frac{1}{1+e^{-WX + b}}f(x)=1+e−WX+b1​$ ReLU():线性修正单元f(x)=max(0,x)f(x) = max(0,x)f(x)=max(0,x) tanh():双曲正切函数f(x)=ex−e−xex+e−xf(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}f(x)=ex+e−xex−e−x​ ELU: PReLU: LReLU:x负半轴斜率比较小的PReLU def sigmoid(A): return 1/(1+np.exp(-A)) 前向线性传播 Z=WX+bZ = WX + bZ=WX+b A=11+e−ZA = \\frac{1}{1+e^-Z}A=1+e−Z1​ def liner_propagation(w, A): Z = np.dot(w, A) A = sigmoid(Z) return Z, A L层的前向传播 是L次linear_propagation的叠加 由于反向传播的需要，这里的L层前向传播需要缓存每一层的A，Z到A_cache and Z_chache def L_layers_propagation(parameters, x): A_cache = list() Z_cache = list() cache = dict() A_cache.append(x) Z_cache.append(x) w = parameters[&quot;w&quot;] layers = parameters['layers'] for i in range(0, len(layers) - 1): Z, A = liner_propagation(w[i], A_cache[i]) assert Z.shape == A.shape Z_cache.append(Z) A_cache.append(A) cache[&quot;A&quot;] = A_cache cache[&quot;Z&quot;] = Z_cache return cache Loss函数（均方误差） 如果要实现任意loss函数，只会影响链式传播哦的最后一项，而不会影响链式传播的中间过程 由于最近DDL多，所以任意loss函数的实现，在寒假实现 def lossFunc(Y_pre, Y_true, method = &quot;MSE&quot;): temp = Y_pre - Y_true return .5 * np.dot(temp, temp.T) 预测函数 input = x output = y y=f(x)y = f(x)y=f(x) y&gt;.5y&gt;.5y&gt;.5 =&gt; y=1y=1y=1 y≤.5y\\le .5y≤.5 =&gt; y=0y= 0y=0 def predict(parameters, x, draw = False): Y = L_layers_propagation(parameters, x)['A'][-1] if draw == False: Y[Y &gt; .5] = 1 Y[Y &lt;= .5] = 0 return Y 反向传播函数 back_propagation 实现反向传播的难点在于求梯度 求梯度的方法如下： dx=f(x)−f(x−Δx)Δxdx = \\frac{f(x) - f(x-\\Delta x)}{\\Delta x}dx=Δxf(x)−f(x−Δx)​ 老师上课提到的这种方法会导致精确度问题 如当在梯度下降算法中设定Δx=η\\Delta x= \\etaΔx=η时该算法退化为 W := W - loss(x-learning_rate) 2.利用求偏导法则，一层一层的求偏导 使用类似 tensorflow 的 Autograd 的计算图，将每一个参数变量加入计算图中，然后可以找到变量之间的关联然后求导(实现过于复杂，对于简单的全链接网络没必要这样做) 使用高等数学中的多变量求导，结合线性代数的矩阵变换进行 实数 对 矩阵 的 链式求导 参考 矩阵求导术（下） 矩阵求导术（上） 引入克罗内克积实现矩阵的链式求导，在注释代码中是没经过花间的Kron积，运算量极大，经过化简得到简单的矩阵表达式 def back_propagation(W, A_prev, dZ): # dZA_prev = np.diag( (A_prev * (1 - A_prev)).T.flatten() ) # I = np.eye(A_prev.shape[1]) # dZ_prev = np.dot(np.dot(dZA_prev,np.kron(I, W.T)), dZ) # I = np.eye(W.shape[0]) # dW = np.dot(np.kron(A_prev, I),dZ) # print(A_prev.shape, W.shape, dZ.shape) dZ_prev = np.multiply( A_prev * (1 - A_prev), np.dot(W.T,dZ)) dW = np.dot(dZ,A_prev.T) return dZ_prev, dW def L_back_propagation(cache, parameters, y): w = parameters[&quot;w&quot;] layers = parameters[&quot;layers&quot;] L = len(layers) - 1 A = cache[&quot;A&quot;] dW_list = list() dZ_prev = A[L] * (1 - A[L]) * (A[L] - y) for i in reversed(range(0, L)): dZ_prev, dW = back_propagation(w[i], A[i], dZ_prev) dW_list.append(dW) cache[&quot;dW&quot;] = dW_list return cache 更新参数 pass def update_parameters(cache, parameters, learning_rate): w = parameters[&quot;w&quot;] layers = parameters[&quot;layers&quot;] L = len(layers) - 1 dW = cache[&quot;dW&quot;] for i in range(L): w[i] -= dW[L-i-1] * learning_rate # b[i] = b[i] - db[len(b)-i-1] * learning_rate parameters[&quot;w&quot;] = w return parameters evaulate 使用准确度作为评估标准 def evaluate(Z, Y): bools = Z == Y accuracy = np.sum(np.reshape(bools,bools.size))/Y.shape[1] return accuracy def evaluate_detial(y_t, y_p): y_test = copy.deepcopy(y_t) y_pred = copy.deepcopy(y_p) print(&quot;The roc_auc_score is {}&quot;.format(roc_auc_score(y_test, y_pred))) tpr,fpr,thresholds = roc_curve(y_test,y_pred) plt.subplot(1,2,1) plt.plot(fpr, tpr) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.0]) plt.title('ROC curve for diabetes classifier') plt.xlabel('False Positive Rate (1 - Specificity)') plt.ylabel('True Positive Rate (Sensitivity)') plt.grid(True) # mean = np.sum(y_pred)/y_pred.shape[0] y_pred[(y_pred &lt; 0.5)] = 0 y_pred[(y_pred &gt;= 0.5)] = 1 # plt.scatter(x_test[0, :], x_test[1, :], c = y_pred[:, 0]) print(&quot;The accuracy is {}&quot;.format(accuracy_score(y_test[:,0], y_pred[:,0]))) # 计算召回， 查全率， 查准率 。。。。 target_names = ['class0','class1'] print(classification_report(y_test,y_pred,target_names = target_names)) ## 绘图 def Plot(X, label, parameters): x_min, x_max = X[0,:].min() - .5, X[0,:].max() + .5 y_min, y_max = X[1,:].min() - .5, X[1,:].max() + .5 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = predict(parameters, np.c_[xx.ravel(), yy.ravel(), np.ones((yy.ravel().shape))].T) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.scatter(X[0,:],X[1,:],c=label[0,:]) load_data 加载数据的管道 def load_data(shape = &quot;circle&quot;): X = np.random.rand(2,200) one = np.ones((1,200)) X = np.vstack((X,one)) C = [] for x in X.T: if shape == &quot;circle&quot;: # 同心圆 if (x[0]-.5)**2 + (x[1]-.5)**2 &lt; .03: C.append(1) elif (x[0]-.5)**2 + (x[1]-.5)**2 &lt; .13: C.append(0) else: C.append(1) elif shape == &quot;xor&quot;: # 四分 if (x[0] - .5)*(x[1] - .5) &gt;=0: C.append(1) else: C.append(0) x_min, x_max = X[0,:].min() - .05, X[0,:].max() + .05 y_min, y_max = X[1,:].min() - .05, X[1,:].max() + .05 h = 0.01 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) test = np.c_[xx.ravel(), yy.ravel(), np.ones((yy.ravel().shape))].T return X, np.array(C).reshape(1,-1), test, (xx,yy) Back Propagation(BP算法的整体框架) 使用梯度下降的优化方法 其他可用的优化方法 SGD 随机梯度下降 MBGD （Mini Batch Gradient Descent）用于样本容量大，内存/现存不够的情况 Momentum 动量梯度下降 Nesterov NAG Adagrad Adaelta Adam def model(X, Y, test,canvs, layers, learning_rate = 0.01, epoch = 1000, detial = True, draw = False): parameters = initial(layers) draw_param = [] for i in range(epoch): cache = L_layers_propagation(parameters, X) cache = L_back_propagation(cache, parameters, Y) parameters = update_parameters(cache, parameters, learning_rate) if i % 500 == 0 and draw == True: # draw_param.append(copy.deepcopy(parameters)) yield predict(parameters, test, draw = True).reshape(canvs[0].shape) if i % 1000 == 0 and detial == True: print(&quot;this is {}th epoch.&quot;.format(i)) Y_predict = predict(parameters, X, draw=False) accuracy = evaluate(Y, Y_predict) print(&quot;accuracy is {}&quot;.format(accuracy)) if draw == False: return parameters 实现累计BP算法 def model_caculate(X, Y, X_test, Y_test, layers, learning_rate = 0.01, epoch = 1000, interval = 50): parameters = initial(layers) Loss = [] loss = None Acc = [] accuracy = None start = time.time() Y_predict = None for i in range(epoch): cache = L_layers_propagation(parameters, X) cache = L_back_propagation(cache, parameters, Y) loss = lossFunc(Y, cache[&quot;A&quot;][-1]) parameters = update_parameters(cache, parameters, learning_rate) if i % interval == 0: Loss.append(loss[0,0]) Y_predict = predict(parameters, X_test, draw=False) accuracy = evaluate(Y_test, Y_predict) Acc.append(accuracy) if i % (interval * 50) == 0: now = time.time() print(&quot;Calculate Dense Net:~$ {}th epoch, accuracy: {}, loss:{}, time:{}-&quot;.format(i, accuracy, loss[0,0], now - start)) start = time.time() return Loss, Acc, Y_predict X, label, test, canvs = load_data() layers = [3,6,9,9,6,1] epoch = 25000 Loss, accuracy, Y_pred = model_caculate(X, label, X, label, layers, learning_rate=0.01, epoch= epoch) #开始画图 x = np.linspace(0, 25000, 500) plt.plot(x, np.array(Loss)/(max(Loss) - min(Loss)), color='green', label='training loss') plt.plot(x, np.array(accuracy)/(max(accuracy) - min(accuracy)), color='red', label='training accuracy') # 显示图例 plt.legend() plt.xlabel('iteration times') plt.ylabel('rate') plt.show() evaluate_detial(label.T, Y_pred.T) Calculate Dense Net:~$ 0th epoch, accuracy: 0.725, loss:21.40100100371333, time:0.0010008811950683594- Calculate Dense Net:~$ 2500th epoch, accuracy: 0.725, loss:19.748958341789084, time:0.7027204036712646- Calculate Dense Net:~$ 5000th epoch, accuracy: 0.725, loss:18.484585855571503, time:0.7009453773498535- Calculate Dense Net:~$ 7500th epoch, accuracy: 0.705, loss:16.377806924887945, time:0.6996634006500244- Calculate Dense Net:~$ 10000th epoch, accuracy: 0.87, loss:10.950215467935237, time:0.7041025161743164- Calculate Dense Net:~$ 12500th epoch, accuracy: 0.88, loss:9.160948492962692, time:0.701648473739624- Calculate Dense Net:~$ 15000th epoch, accuracy: 0.885, loss:8.84026852930535, time:0.6946816444396973- Calculate Dense Net:~$ 17500th epoch, accuracy: 0.965, loss:6.105461022050368, time:0.685279369354248- Calculate Dense Net:~$ 20000th epoch, accuracy: 0.99, loss:2.3057686501449832, time:0.7090015411376953- Calculate Dense Net:~$ 22500th epoch, accuracy: 0.99, loss:0.958385284049958, time:0.6965689659118652- The roc_auc_score is 1.0 The accuracy is 1.0 precision recall f1-score support class0 1.00 1.00 1.00 55 class1 1.00 1.00 1.00 145 micro avg 1.00 1.00 1.00 200 macro avg 1.00 1.00 1.00 200 weighted avg 1.00 1.00 1.00 200 实现标准BP算法（也就是Mini-batch = 1，这样的话在python下使用for循环喂数据会非常慢） def model_std(X, Y, X_test, Y_test, layers, learning_rate = 0.01, epoch = 1000, interval = 50): parameters = initial(layers) Loss = [] Acc = [] accuracy = None loss = None Y_pred = None start = time.time() for i in range(epoch): for j in range(0, X.shape[1]): cache = L_layers_propagation(parameters, X[:, j].reshape(3, -1)) cache = L_back_propagation(cache, parameters, Y[:, j].reshape(1, -1)) parameters = update_parameters(cache, parameters, learning_rate) if i % interval == 0: Y_pred = predict(parameters, X_test, draw = False) loss = lossFunc(Y_test, Y_pred)[0,0] Loss.append(loss) accuracy = evaluate(Y_test, Y_pred) Acc.append(accuracy) if i % (interval * 20) == 0: now = time.time() print(&quot;Stander Dense Net:~$ {}th epoch, accuracy: {}, loss:{}, time:{}&quot;.format(i, accuracy, loss, now - start)) start = time.time() return Loss, Acc, Y_pred X, label, test, canvs = load_data() layers = [3,6,9,9,6,1] epoch = 33300 Loss,accuracy, Y_pred = model_std(X, label, X, label, layers, learning_rate=0.01, epoch= epoch) #开始画图 x = np.linspace(0, 25000, epoch/50) plt.plot(x, np.array(Loss)/(max(Loss) - min(Loss)), color='green', label='training loss') plt.plot(x, np.array(accuracy)/(max(accuracy) - min(accuracy)), color='red', label='training accuracy') # 显示图例 plt.legend() plt.xlabel('iteration times') plt.ylabel('rate') plt.show() evaluate_detial(label.T, Y_pred.T) Stander Dense Net:~$ 0th epoch, accuracy: 0.65, loss:35.0, time:0.018254756927490234 动态可视化训练过程 def main(epoch= 50000, detial=True, draw=False): X, label, test, canvs = load_data() layers = [3,6,9,9,6,1] if draw == True: for im in model(X, label, test, canvs, layers, learning_rate=0.01, epoch= epoch, detial = detial, draw = True): plt.clf() display.clear_output(wait=True) plt.contourf(canvs[0] ,canvs[1], im, cmap=plt.cm.Spectral) plt.scatter(X[0,:], X[1,:],c=label[0,:]) plt.pause(0.01) else: parameters = model(X, label, test, canvs, layers, learning_rate=0.01, epoch= 30000, detial = detial, draw = draw) Plot(X, label, parameters) main(epoch= 20000, detial=False, draw=True) 西瓜数据集3.0α\\alphaα上的分类 导入数据 import numpy as np import copy def load(): data = np.array([[0.697, 0.460, 1], [0.774, 0.376, 1], [0.634, 0.264, 1], [0.608, 0.318, 1], [0.556, 0.215, 1], [0.403, 0.237, 1], [0.481, 0.149, 1], [0.437, 0.211, 1], [0.666, 0.091, 0], [0.243, 0.267, 0], [0.245, 0.057, 0], [0.343, 0.099, 0], [0.639, 0.161, 0], [0.657, 0.198, 0], [0.360, 0.370, 0], [0.593, 0.042, 0], [0.719, 0.103, 0]]) Y = copy.deepcopy(data[:, 2].reshape(-1, 1).T) data[:, 2] = 1 X = copy.deepcopy(data.T) X_test = copy.deepcopy(np.c_[X[:, 0:2],X[:, -3:-1]]) Y_test = copy.deepcopy(np.c_[Y[:, 0:2],Y[:, -3:-1]]) X_train = copy.deepcopy(X[:, 2:-2]) Y_train = copy.deepcopy(Y[:, 2:-2]) return X_train, Y_train, X_test, Y_test, X, Y X_train, Y_train, X_test, Y_test, X, Y = load() print(Y.shape) 累计BP X_train, Y_train, X_test, Y_test, X, Y = load() X_train.shape, Y_train.shape, X_test.shape, Y_test.shape layers = [3,5,1] epoch = 5500 interval = 1 Loss, accuracy, Y_pred = model_caculate(X_train, Y_train, X_test, Y_test, layers, learning_rate=0.1, epoch= epoch, interval = interval) #开始画图 x = np.linspace(0, 25000, epoch/interval) plt.plot(x, np.array(Loss)/(max(Loss) - min(Loss)), color='green', label='training loss') plt.plot(x, np.array(accuracy)/(max(accuracy) - min(accuracy)), color='red', label='training accuracy') # 显示图例 plt.legend() plt.xlabel('iteration times') plt.ylabel('rate') plt.show() evaluate_detial(Y_test.T, Y_pred.T) X, label, test, canvs = load_data() X.shape, label.shape 标准BP算法 由于实现了累计误差的算法， 所以可以将样样本拆分得到mini-batch的神经网络就是标准BP算法 layers = [3,5,1] epoch = 5500 interval = 1 Loss,accuracy, Y_pred = model_std(X_train, Y_train, X_test, Y_test, layers, learning_rate=0.01, epoch= epoch) #开始画图 x = np.linspace(0, 25000, epoch/50) plt.plot(x, np.array(Loss)/(max(Loss) - min(Loss)), color='green', label='training loss') plt.plot(x, np.array(accuracy)/(max(accuracy) - min(accuracy)), color='red', label='training accuracy') # 显示图例 plt.legend() plt.xlabel('iteration times') plt.ylabel('rate') plt.show() evaluate_detial(Y_test.T, Y_pred.T) 模型比较 标准BP算法收敛更快 累计BP在python可以通过矩阵运算向量化加速,在迭代过程中震荡变小 累计BP 更占用内存，而标准BP可以一边读一边运算，收敛速度变慢但是梯度方向更准确 Minibatch = n，可以通过调节n的大小使其达到一个合适的值，其收敛速度和迭代的准确性都能提高！ 神经网络的优势 相对于决策树，线性模型，神经网络可以通过增加层数/增加神经元的方式拟合任意非线性函数 在之前的同心圆数据集中，神经网络的表达能力远远优于其他模型 \\begin{split} &amp; C(F , G) = \\frac{F \\cdot G+(1 - F\\otimes G)}{2}\\ &amp; F\\cdot G = \\vee_U(\\mu_F(u_i)\\land\\mu_G(u_i)) \\ &amp; F\\otimes G = \\vee_U(\\mu_F(u_i)\\vee\\mu_G(u_i)) \\end{split} ","link":"https://xuaii.github.io/post/neural-network/"},{"title":"SVM","content":"svm.html 线性核SVM 和 高斯核SVM SMO 算法解析 最优化目标 (1)max⁡α=∑i=1mαi−∑i=1m∑j=1mαiαjyiyjxiTxj\\max_{\\alpha} =\\sum_{i=1}^{m}\\alpha_i - \\sum_{i=1}^{m} \\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\tag{1} αmax​=i=1∑m​αi​−i=1∑m​j=1∑m​αi​αj​yi​yj​xiT​xj​(1) (2) s.t. αi≥0,∑i=1Nαiyi=1\\text{ s.t. } \\qquad \\alpha_i \\ge 0,\\quad \\sum_{i=1}^{N}\\alpha_iy_i = 1 \\tag{2} s.t. αi​≥0,i=1∑N​αi​yi​=1(2) 约束条件 算法原理 我们可以先确定 两个αi,αj\\alpha_i,\\alpha_jαi​,αj​. 在例子中设i=1,j=2i=1 , \\quad j=2i=1,j=2. 此时最大化目标： (3)argmax⁡α1,α2W(α1,α2)=α1+α2−12K1,1y12α12−12K2,2y22α22−K1,2y1y2α1α2−y1α1∑i=3mαiyiKi,1−y2α2∑i=3mαiyiKi,2+Carg\\max_{\\alpha_1, \\alpha_2}W(\\alpha_1, \\alpha_2) = \\alpha_1 + \\alpha_2 - \\frac{1}{2}K_{1,1}y_1^2\\alpha_1^2 - \\frac{1}{2}K_{2,2}y_2^2\\alpha_2^2 - K_{1,2}y_1y_2\\alpha_1\\alpha_2 - y_1\\alpha_1\\sum_{i=3}^{m}\\alpha_iy_iK_{i,1} - y_2\\alpha_2\\sum_{i=3}^{m}\\alpha_iy_iK_{i,2} + C \\tag{3} argα1​,α2​max​W(α1​,α2​)=α1​+α2​−21​K1,1​y12​α12​−21​K2,2​y22​α22​−K1,2​y1​y2​α1​α2​−y1​α1​i=3∑m​αi​yi​Ki,1​−y2​α2​i=3∑m​αi​yi​Ki,2​+C(3) 根据(2) α1y1+α2y2=−∑i=3Nαiyi=η\\alpha_1y_1 + \\alpha_2y_2 = -\\sum_{i=3}^{N}\\alpha_iy_i = \\eta α1​y1​+α2​y2​=−i=3∑N​αi​yi​=η 两边同时乘以y1,由于yi2=1y_1,由于y_i^2 = 1y1​,由于yi2​=1 (4)α1=ηy1−α2y1y2\\alpha_1 = \\eta y_1 - \\alpha_2y_1y_2 \\tag{4} α1​=ηy1​−α2​y1​y2​(4) 令： (5)v1=∑i=3NαiyiKi,1,v2=∑i=3NαiyiKi,2v_1 = \\sum^{N}_{i=3}\\alpha_iy_iK_{i,1}, \\quad v_2 = \\sum^{N}_{i=3}\\alpha_iy_iK_{i,2} \\tag{5} v1​=i=3∑N​αi​yi​Ki,1​,v2​=i=3∑N​αi​yi​Ki,2​(5) 将 (4), (5) 带入(3) (6)W(α2)=−12K1,1(η−α2y2)2−12K2,2α22−K1,2y2α2(η−α2y2)−v1(η−α2y2)−v2y2α2+α1+α2+CW(\\alpha_2) = - \\frac{1}{2}K_{1,1}(\\eta - \\alpha_2y_2)^2 - \\frac{1}{2}K_{2,2}\\alpha_2^2 - K_{1,2}y_2\\alpha_2(\\eta - \\alpha_2y_2) - v_1(\\eta - \\alpha_2y_2) - v_2y_2\\alpha_2 + \\alpha_1 + \\alpha_2 + C \\tag{6} W(α2​)=−21​K1,1​(η−α2​y2​)2−21​K2,2​α22​−K1,2​y2​α2​(η−α2​y2​)−v1​(η−α2​y2​)−v2​y2​α2​+α1​+α2​+C(6) 由于需要更新α2\\alpha_2α2​所以令dWdα=0\\frac{dW}{d\\alpha} = 0dαdW​=0 (7)∂W(α2)∂α2=−α2(K1,1+K2,2−2K1,2)+K1,1ηy2−K1,2ηy2+v1y2−v2y2−y1y2+y22=0\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2} = -\\alpha_2(K_{1,1} + K_{2,2} - 2K_{1,2}) + K_{1,1}\\eta y_2 - K_{1,2}\\eta y_2 + v_1y_2 - v_2y_2 - y_1y_2 + y_2^2 = 0 \\tag{7} ∂α2​∂W(α2​)​=−α2​(K1,1​+K2,2​−2K1,2​)+K1,1​ηy2​−K1,2​ηy2​+v1​y2​−v2​y2​−y1​y2​+y22​=0(7) 对(7)式变形，使得α2new\\alpha_2^newα2n​ew能被α2old\\alpha_2^oldα2o​ld表示(而不是用不方便的\\eta)： SVM预测值如下(该式子不需要保留所有的x，因为很多无关的x的alpha都为0，因此alpha需要初始化为0)： (8)f(x)=∑i=1NαiyiK(xi,x)+bf(x) = \\sum_{i=1}^{N}\\alpha_iy_iK(x_i,x) + b \\tag{8} f(x)=i=1∑N​αi​yi​K(xi​,x)+b(8) 则v可以表示为： v1=∑i=3NαiyiK1,i=f(x1)−α1y1K1,1−α2y2K1,2−bv_1 = \\sum^{N}_{i=3}\\alpha_iy_iK_{1,i} = f(x_1) - \\alpha_1y_1K_{1,1} - \\alpha_2y_2K_{1,2} - b v1​=i=3∑N​αi​yi​K1,i​=f(x1​)−α1​y1​K1,1​−α2​y2​K1,2​−b v2=∑i=3NαiyiK2,i=f(x2)−α1y1K1,2−α2y2K2,2−bv_2 = \\sum^{N}_{i=3}\\alpha_iy_iK_{2,i} = f(x_2) - \\alpha_1y_1K_{1,2} - \\alpha_2y_2K_{2,2} - b v2​=i=3∑N​αi​yi​K2,i​=f(x2​)−α1​y1​K1,2​−α2​y2​K2,2​−b 已知： α1=(η−α2y2)y2\\alpha_1= (\\eta - \\alpha_2y_2)y_2 α1​=(η−α2​y2​)y2​ 可得到： (9)v1−v2=f(x1)−f(x2)−K1,1η+K1,2η+α2y2(K1,1+K2,2−2K1,2)v_1 - v_2 = f(x_1) - f(x_2) - K_{1,1}\\eta + K_{1,2}\\eta + \\alpha_2y_2(K_{1,1}+K_{2,2}-2K_{1,2}) \\tag{9} v1​−v2​=f(x1​)−f(x2​)−K1,1​η+K1,2​η+α2​y2​(K1,1​+K2,2​−2K1,2​)(9) 将(9)带入(7): ∂W(α2)∂α2=−(K1,1+K2,2−2K1,2)α2new+(K1,1+K2,2−2K1,2)α2old+y2(y2−y1+f(x1)−f(x2))\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2} = -(K_{1,1} + K_{2,2} - 2K_{1,2})\\alpha_2^{new} + (K_{1,1} + K_{2,2}- 2K_{1,2})\\alpha_2^{old} + y_2(y_2 - y_1 + f(x_1) - f(x_2)) ∂α2​∂W(α2​)​=−(K1,1​+K2,2​−2K1,2​)α2new​+(K1,1​+K2,2​−2K1,2​)α2old​+y2​(y2​−y1​+f(x1​)−f(x2​)) 记误差项Ei=f(xi)−yiE_i = f(x_i) - y_iEi​=f(xi​)−yi​ 令θ=K1,1,+K2,2−2K1,2\\theta = K_{1,1,}+K_{2,2}-2K_{1,2}θ=K1,1,​+K2,2​−2K1,2​ 可以得到最终表达式： ∂W(α2)∂α2=−θα2new+θα2old+y2(E1−E2)=0\\frac{\\partial W(\\alpha_2)}{\\partial \\alpha_2} = -\\theta\\alpha_2^{new}+ \\theta\\alpha_2^{old}+y2(E_1-E_2) = 0 ∂α2​∂W(α2​)​=−θα2new​+θα2old​+y2(E1​−E2​)=0 得到： (10)α2new=α2old+y2(E1−E2)θ\\alpha_2^{new} = \\alpha_2^{old} + \\frac{y_2(E_1- E_2)}{\\theta} \\tag{10} α2new​=α2old​+θy2​(E1​−E2​)​(10) 到了这里，如果要更新参数，仅需计算E，和\\theta 然后计算\\alpha2 原始解的修剪 现在考虑约束条件 上面通过对一元函数求极值的方式更新了参数得到了α2new,unclipped\\alpha_2^{new,unclipped}α2new,unclipped​ 现在通过对原始解的修正得到α2new,cilpped\\alpha_2^{new,cilpped}α2new,cilpped​ (10)α2new,unclipped=α2old+y2(E1−E2)θ\\alpha_2^{new,unclipped} = \\alpha_2^{old} + \\frac{y_2(E_1- E_2)}{\\theta} \\tag{10} α2new,unclipped​=α2old​+θy2​(E1​−E2​)​(10) 约束条件（画图分析）： ify1≠y2:if \\quad y_1 \\ne y_2 :ify1​​=y2​: 上界：L=max(0,α2old−α1old)上界：\\qquad L = max(0, \\quad \\alpha_2^{old} - \\alpha_1^{old}) 上界：L=max(0,α2old​−α1old​) 下界：H=max(C,C+α2old−α1old)下界：\\qquad H = max(C, \\quad C+\\alpha_2^{old} - \\alpha_1^{old}) 下界：H=max(C,C+α2old​−α1old​) elify1=y2:elif \\quad y_1 = y_2 :elify1​=y2​: 上界：L=max(0,α2old+α1old−C)上界：\\qquad L = max(0, \\quad \\alpha_2^{old} + \\alpha_1^{old} -C) 上界：L=max(0,α2old​+α1old​−C) 下界：H=max(C,α2old+α1old)下界：\\qquad H = max(C, \\quad \\alpha_2^{old} + \\alpha_1^{old}) 下界：H=max(C,α2old​+α1old​) 更新参数都可以计算：α1new\\alpha_1^{new}α1new​ 由： α1oldy1+α2oldy2=α1newy1+α2newy2\\alpha_1^{old}y_1 + \\alpha_2^{old}y_2 = \\alpha_1^{new}y_1 + \\alpha_2^{new}y_2 α1old​y1​+α2old​y2​=α1new​y1​+α2new​y2​ 得到： α1new=α1old+y1y2(α2old−α2new)\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new}) α1new​=α1old​+y1​y2​(α2old​−α2new​) 最后由式(6.17)-&gt;(6.18)可得到： b=1m∑s=1m[1/ys−∑mi=1αiyixiTxs]b = \\frac{1}{m}\\sum_{s=1}^{m}[1/y_s - \\sum_{m}^{i=1}\\alpha_iy_ix_i^Tx_s] b=m1​s=1∑m​[1/ys​−m∑i=1​αi​yi​xiT​xs​] 3.启发式选择变量 上述分析是在从N个变量中已经选出两个变量进行优化的方法，下面分析如何高效地选择两个变量进行优化，使得目标函数下降的最快。 第一个变量的选择 第一个变量的选择称为外循环，首先遍历整个样本集，选择违反KKT条件的αi\\alpha_iαi​作为第一个变量 接着依据相关规则选择第二个变量(见下面分析),对这两个变量采用上述方法进行优化。 当遍历完整个样本集后，遍历非边界样本(0&lt;αi&lt;C)(0&lt;α_i&lt;C)(0&lt;αi​&lt;C)中违反KKT的αi\\alpha_iαi​作为第一个变量，同样依据相关规则选择第二个变量，对此两个变量进行优化。当遍历完非边界样本集后，再次回到遍历整个样本集中寻找，即在整个样本集与非边界样本集上来回切换，寻找违反KKT条件的αiαi作为第一个变量。直到遍历整个样本集后，没有违反KKT条件αi\\alpha_iαi​，然后退出。 边界上的样本对应的αi=0\\alpha_i = 0αi​=0或者αi=C\\alpha_i = Cαi​=C，在优化过程中很难变化，然而非边界样本0&lt;αi&lt;C0&lt;α_i&lt;C0&lt;αi​&lt;C会随着对其他变量的优化会有大的变化。 KTT条件 αi=0⟺y(i)(wTx(i)+b)≥1\\alpha_i = 0 \\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b)\\ge1 αi​=0⟺y(i)(wTx(i)+b)≥1 αi=C⟺y(i)(wTx(i)+b)≤1\\alpha_i = C \\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b)\\le 1 αi​=C⟺y(i)(wTx(i)+b)≤1 0&lt;αi&lt;C⟺y(i)(wTx(i)+b)=10 \\lt \\alpha_i \\lt C \\Longleftrightarrow y^{(i)}(w^Tx^{(i)}+b) =1 0&lt;αi​&lt;C⟺y(i)(wTx(i)+b)=1 第二个变量的选择 SMO称第二个变量的选择过程为内循环，假设在外循环中找个第一个变量记为α1\\alpha_1α1​，第二个变量的选择希望能使α2\\alpha_2α2​有较大的变化，由于α2\\alpha_2α2​是依赖于∣E1−E2∣|E1−E2|∣E1−E2∣,当E1E1E1为正时，那么选择最小的EiEiEi作为E2E2E2,如果E1E1E1为负，选择最大EiEiEi作为E2E2E2，通常为每个样本的EiEiEi保存在一个列表中，选择最大的∣E1−E2∣|E1−E2|∣E1−E2∣来近似最大化步长。 有时按照上述的启发式选择第二个变量，不能够使得函数值有足够的下降，这时按下述步骤: 1.首先在非边界集上选择能够使函数值足够下降的样本作为第二个变量， 2.如果非边界集上没有，则在整个样本集上选择第二个变量， 3.如果整个样本集依然不存在，则重新选择第一个变量。 参考 机器学习算法实践-SVM中的SMO算法 详细推导的SMO更新参数和修剪结果的公式， 优化算法效率：例如只需最后计算b。。。。 浅谈最优化问题的KKT条件 import numpy as np import matplotlib.pyplot as plt import copy from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report, roc_auc_score np.random.seed(1) 定义训练集 Xtrain∈Rn×mX_{train} \\in \\R^{n \\times m} Xtrain​∈Rn×m xi∈Rn,即每一个样本有n个特征x_i \\in \\R^n,即每一个样本有n个特征 xi​∈Rn,即每一个样本有n个特征 label∈{−1,1},二分类问题label \\in \\{-1,1\\},二分类问题 label∈{−1,1},二分类问题 多种核函数 def LinearKernel(x_i, x_j): if x_i.shape[1] != 1: raise Exception(&quot;x_i is wrong shape!&quot;) if x_j.shape[1] != 1: raise Exception(&quot;x_j is wrong shape!&quot;) return np.dot(x_i.T, x_j) def GaussKernel(x_i, x_j, sigmoid): if x_i.shape[1] != 1: raise Exception(&quot;x_i is wrong shape!&quot;) if x_j.shape[1] != 1: raise Exception(&quot;x_j is wrong shape!&quot;) if sigmoid &lt;= 0: raise Exception(&quot;sigmoid must be a positive number&quot;) return np.exp(-np.dot((x_i - x_j).T, (x_i - x_j))/(2 * sigmoid**2)) def PolyKernel(x_i, x_j, d): if x_i.shape[1] != 1: raise Exception(&quot;x_i is wrong shape!&quot;) if x_j.shape[1] != 1: raise Exception(&quot;x_j is wrong shape!&quot;) if d &lt; 0: raise Exception(&quot;d must be a Semi-positive number&quot;) return LinearKernel(x_i, x_j)**d def LaplaceKernel(x_i, x_j, s): if x_i.shape[1] != 1: raise Exception(&quot;x_i is wrong shape!&quot;) if x_j.shape[1] != 1: raise Exception(&quot;x_j is wrong shape!&quot;) if s &lt;= 0: raise Exception(&quot;sigmoid must be a positive number&quot;) # print(np.exp(-np.sqrt(np.dot((x_i - x_j).T, (x_i - x_j))) / sigmoid)) return np.exp(-np.sqrt(np.dot((x_i - x_j).T, (x_i - x_j))) / s)[0][0] def SigmoidKernel(x_i, x_j, beta, theta): if x_i.shape[1] != 1: raise Exception(&quot;x_i is wrong shape!&quot;) if x_j.shape[1] != 1: raise Exception(&quot;x_j is wrong shape!&quot;) if theta &gt;= 0: raise Exception(&quot;theta must be a negetive number&quot;) if beta &lt;= 0: raise Exception(&quot;theta must be a positive number&quot;) return np.tanh(beta * LinearKernel(x_i, x_j) + theta) def K(x_i, x_j, kernel = &quot;linear&quot;, s = .5, beta = 1, theta = -1, d = 2): # print(x_i.shape, x_j.shape) kernel = kernel.lower() try: if kernel == 'linear': return LinearKernel(x_i, x_j) elif kernel == 'gauss': return GaussKernel(x_i, x_j, s) elif kernel == 'poly': return PolyKernel(x_i, x_j, d) elif kernel == 'laplace': return LaplaceKernel(x_i, x_j, s) elif kernel == 'sigmoid': return SigmoidKernel(x_i, x_j, beta, theta) except Exception as err: print('An exception happened: ' + str(err)) if __name__ == &quot;__main__&quot;: x_i = np.ones((10,1)) x_j = np.random.randn(10,1) print(np.exp(-np.sqrt(np.dot((x_i - x_j).T, (x_i - x_j))) / 1)) print(K(x_i,x_j,kernel=&quot;Laplace&quot;, s=1)) [[0.005973]] 0.005973003723489288 生成同心圆数据 import numpy as np def load(type = &quot;circle&quot;): x = np.random.uniform(-1,1,(100,1)) y = np.random.uniform(-1,1,(100,1)) x_train = np.c_[x,y].T print(x_train.shape) label_train = np.ones((100,1)) mask = (x**2 + y**2 ) &lt; .5 label_train[mask] = -1 x1 = np.random.uniform(-1,1,(100,1)) y1 = np.random.uniform(-1,1,(100,1)) x_test = np.c_[x1,y1].T print(x_test.shape) label_test = np.ones((100,1)) mask = (x1**2 + y1**2 ) &lt; .5 label_test[mask] = -1 return x_train, label_train, x_test, label_test if __name__ == &quot;__main__&quot;: x_train, label_train, x_test, label_test = load() print(x_train.shape, label_train.shape, x_test.shape, label_test.shape) (2, 100) (2, 100) (2, 100) (100, 1) (2, 100) (100, 1) 参照第一个SMO推导笔记需要实现以下功能： 初始化参数（主要是\\alpha） 由于采用缓存（用一个列表记录）所有的Ei=yi−f(xi)E_i = y_i - f(x_i)Ei​=yi​−f(xi​) 所以每一次更新参数α\\alphaα后都需要更新Ei,EjE_i, E_jEi​,Ej​ 因为由KKT条件约束，所以需要对最后的结果进行修剪 所以需要实现 clip() f(indexi)用于实现对训练集的某一个特征向量计算预测值f(index_i)用于实现对训练集的某一个特征向量计算预测值f(indexi​)用于实现对训练集的某一个特征向量计算预测值 predict(xi)用于预测任意给定的特征向量xipredict(x_i)用于预测任意给定的特征向量x_ipredict(xi​)用于预测任意给定的特征向量xi​ select()函数的内循环和外循环用于选择最不满足KKT条件的并且能够使得更新效果最明显的αiαj\\alpha_i \\quad \\alpha_jαi​αj​ loss()用于计算E update_a2()用于更新α2\\alpha_2α2​ update_a1()使用α2\\alpha_2α2​和α1old\\alpha_1^{old}α1old​更新α1\\alpha_1α1​ SMO()是smo算法的主循环 以上函数与SMO算法紧密相关，为了方便参数，数据集，的传输将他们放到一个SMO类中实现 class SMO: def __init__(self, x, y, kernel = &quot;Laplace&quot;, C=10): self.x = copy.deepcopy(x) self.y = copy.deepcopy(y) self.m = x.shape[1] self.n = x.shape[0] self.kernel = kernel self.C = C def init(self): # self.w = np.random.randn(self.n, 1) self.b = np.zeros((1, 1)) self.a = np.zeros((self.m, 1)) E = [] for i in range(0, self.m): E.append(self.loss(i)) self.E = E # 每次更新参数后需要更新E def updateE(self, i, j): self.E[i] = self.loss(i) self.E[j] = self.loss(j) def clip(self, index_1, index_2, old_1, old_2): # get H, L alpha = self.a[index_2,:][0] if self.y[index_1, :] != self.y[index_2, :]: L = max(0.0, old_2[0] - old_1[0]) H = max(self.C, self.C + old_2[0] - old_1[0]) else: # print(&quot;----------&quot; , self.C, old_2[0], self.C + old_2[0],'-------------') L = max(0.0, old_2[0] + old_1[0] - self.C) H = max(self.C, old_2[0] + old_1[0]) if alpha &lt; L: return L elif alpha &gt; H: return H else: return alpha def f(self, x_index): k = np.zeros(self.y.shape) for i in range(0, self.m): k[i,:] = copy.deepcopy(K(self.x[:,i].reshape(-1,1), self.x[:,x_index].reshape(-1,1), kernel = self.kernel)) return np.sum(self.a * self.y * k)+self.b # 此处是选择第二个变量，第一个变量仅需要无脑便利就好了 # 需要传入第一个变量的index以计算E_1 # 是否可以将select写成一个生成器？ def select(self): for i in range(0, self.m): # label用于标志是否违反KKT label = False temp = self.y[i, :] * self.f(i) if self.a[i, :] == 0: if temp &lt; 1 : label = True elif 0 &lt; self.a[i, :] and self.a[i, :] &lt; self.C: if temp &gt; 1: label = True else: if temp != 1: label = True # 如果违反KKT条件，进入内循环选择第二个a if label == True and self.E[i] &lt;= 0: j = np.argmax(np.array(self.E)) yield i,j if label == True and self.E[i] &gt;0: j = np.argmin(np.array(self.E)) yield i,j # 需要得到边界变量的下标值 boolen = (self.a &gt; 0) &amp; (self.a &lt; self.C) edgeVar = [] for i in range(0, len(boolen)): if boolen[i] == True: edgeVar.append(i) for i in edgeVar: # label用于标志是否违反KKT label = False temp = self.y[i, :] * self.f(i) if self.a[i, :] == 0: if temp &lt; 1 : label = True elif 0 &lt; self.a[i, :] and self.a[i, :] &lt; self.C: if temp &gt; 1: label = True else: if temp != 1: label = True # 如果违反KKT条件，进入内循环选择第二个a if label == True and self.E[i] &lt;= 0: j = np.argmax(np.array(self.E)) yield i,j if label == True and self.E[i] &gt;0: j = np.argmin(np.array(self.E)) yield i,j # E_i = f(x_i) - y_i def loss(self, index): return self.f(index) - self.y[index, :] def update_a2(self, alpha2_old, index_1, index_2): theta = K(self.x[:,index_1].reshape(-1,1), self.x[:,index_1].reshape(-1,1), kernel = self.kernel) + K(self.x[:,index_2].reshape(-1,1), self.x[:,index_2].reshape(-1,1), kernel = self.kernel) - 2*K(self.x[:,index_1].reshape(-1,1), self.x[:,index_2].reshape(-1,1), kernel = self.kernel) if theta &lt; 0.001 and theta &gt;= 0: theta = 0.001 if theta &gt; -0.001 and theta &lt;0: theta = 0.001 dE = self.E[index_1] - self.E[index_2] # print(&quot;in update_a2 theta:{}, dE:{}, y2:{}&quot;.format(theta, dE, self.y[index_2])) return alpha2_old + (self.y[index_2] * dE) / theta def update_a1(self, alpha1_old, alpha2_old, alpha2_new, index_1, index_2): return alpha1_old + self.y[index_1] * self.y[index_2] * (alpha2_old - alpha2_new) def SMO(self, maxtimes): for i in range(0, maxtimes): if i % 10 == 0: print(&quot;this is {}th loop.&quot;.format(i)) for i, j in self.select(): temp = copy.deepcopy(self.a[j]) self.a[j] = self.update_a2(self.a[j], i, j) self.a[j] = self.clip(i, j, self.a[i], temp) self.a[i] = self.update_a1(self.a[i], temp, self.a[j], i, j) self.updateE(i, j) # 更新b self.b = np.sum(1/self.y)/self.m for i in range(0, self.m): self.b -= (self.f(i)/self.m) print(self.b) def predict(self, x): k = np.zeros(self.y.shape) for i in range(0, self.m): k[i,:] = copy.deepcopy(K(self.x[:,i].reshape(-1,1), x.reshape(-1,1), kernel = self.kernel)) # return k return (np.sum(self.a * self.y * k) + self.b) def predictAll(self, x_test): # get prected y y_pred = np.zeros(y_test.shape) for i in range(0, x_test.shape[1]): y_pred[i, :] = self.predict(x_test[:, i].reshape(2, -1)) return y_pred def evaluate(y_t, y_p): y_test = copy.deepcopy(y_t) y_pred = copy.deepcopy(y_p) print(&quot;The roc_auc_score is {}&quot;.format(roc_auc_score(y_test, y_pred))) tpr,fpr,thresholds = roc_curve(y_test,y_pred) plt.subplot(1,2,1) plt.plot(fpr, tpr) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.0]) plt.title('ROC curve for diabetes classifier') plt.xlabel('False Positive Rate (1 - Specificity)') plt.ylabel('True Positive Rate (Sensitivity)') plt.grid(True) # mean = np.sum(y_pred)/y_pred.shape[0] y_pred[(y_pred &lt; 0)] = -1 y_pred[(y_pred &gt;= 0)] = 1 # plt.scatter(x_test[0, :], x_test[1, :], c = y_pred[:, 0]) print(&quot;The accuracy is {}&quot;.format(accuracy_score(y_test[:,0], y_pred[:,0]))) # 计算召回， 查全率， 查准率 。。。。 target_names = ['class0','class1'] print(classification_report(y_test,y_pred,target_names = target_names)) def draw_svm(smo): xx, yy = np.meshgrid(np.arange(-1, 1, 0.01), np.arange(-1, 1, 0.01)) zz = np.zeros((200, 200)) for i in range(0, 200): for j in range(0, 200): temp = np.array([xx[i, j], yy[i, j]]).reshape(2, -1) res = smo.predict(temp) # print(res) zz[i, j] = res # print(zz[i, j]) zz[(zz &lt; 0)] = -1 zz[(zz &gt;= 0)] = 1 plt.subplot(1,2,2) plt.contourf(xx, yy, zz) # # print(x[0,:].shape, x[1,:].shape, y.shape) plt.scatter(smo.x[0,:].reshape(1, -1), smo.x[1,:].reshape(1, -1), c = 0 - smo.y.T) plt.show() 导入数据 此处生成同心圆数据，以区别线性核和非线性核之间的区别 x_train, y_train, x_test, y_test = load() # plt.scatter(x_train[0, :], x_train[1, :], c = y_train[:, 0]) (2, 100) (2, 100) 使用高斯核的SVM 此处σ=0.5\\sigma = 0.5σ=0.5,取值为1时，准确率就很低了，不知道怎么回事（先补一下核函数更多的技术了， 狗头.jpg） smo = SMO(x_train, y_train, C = 2, kernel = &quot;gauss&quot;) smo.init() smo.SMO(110) y_pred = smo.predictAll(x_test) evaluate(y_test, y_pred) draw_svm(smo) this is 0th loop. this is 10th loop. this is 20th loop. this is 30th loop. this is 40th loop. this is 50th loop. this is 60th loop. this is 70th loop. this is 80th loop. this is 90th loop. this is 100th loop. 1.0542578147143928 The roc_auc_score is 0.982048143614851 The accuracy is 0.89 precision recall f1-score support class0 0.80 1.00 0.89 43 class1 1.00 0.81 0.89 57 accuracy 0.89 100 macro avg 0.90 0.90 0.89 100 weighted avg 0.91 0.89 0.89 100 使用线性核的SVM smo = SMO(x_train, y_train, C = 2, kernel = &quot;linear&quot;) smo.init() smo.SMO(110) y_pred = smo.predictAll(x_test) evaluate(y_test, y_pred) draw_svm(smo) this is 0th loop. this is 10th loop. this is 20th loop. this is 30th loop. this is 40th loop. this is 50th loop. this is 60th loop. this is 70th loop. this is 80th loop. this is 90th loop. this is 100th loop. 0.27041462716608683 The roc_auc_score is 0.4618523051815585 The accuracy is 0.48 precision recall f1-score support class0 0.41 0.49 0.45 43 class1 0.55 0.47 0.51 57 accuracy 0.48 100 macro avg 0.48 0.48 0.48 100 weighted avg 0.49 0.48 0.48 100 调用之前写过的决策树C4.5 from dt import buildTree, Node, predictAll, predict, draw_tree import pandas as pd X_train = np.r_[x_train, y_train.T] X_train = pd.DataFrame(X_train.T, columns=[&quot;x&quot;,'y',&quot;label&quot;]) X_test = np.r_[x_test, y_test.T] X_test = pd.DataFrame(X_test.T, columns=[&quot;x&quot;,'y',&quot;label&quot;]) Y_test = np.array(list(X_test[&quot;label&quot;])).reshape(-1,1) root = Node(Dataset=X_train, attrList = ['x','y']) root = buildTree(root) Y_pred = predictAll(root ,X_test) evaluate(Y_test, Y_pred) draw_tree(root) plt.scatter(X_train[&quot;x&quot;], X_train[&quot;y&quot;], c = 0 - X_train[&quot;label&quot;]) The roc_auc_score is 0.7105263157894737 The accuracy is 0.67 precision recall f1-score support class0 0.57 1.00 0.72 43 class1 1.00 0.42 0.59 57 accuracy 0.67 100 macro avg 0.78 0.71 0.66 100 weighted avg 0.81 0.67 0.65 100 &lt;matplotlib.collections.PathCollection at 0x7f523e323310&gt; 分析 对于同心圆这种线性不可分的数据，使用linear kernel只能得到约50/100的准确度 使用决策树，这里由于决策变量只有x，y所以，这里的决策树虽多两层，对于特征向量较小的数据集不友好 使用拉普拉斯核（测试时用的拉普拉斯核，准确率略高于高斯核） 或者 高斯核在该种线性不可分的数据集中起到了很好的升维作用 使用神经网络可以得到很好的效果（在神经网络实现的时候，测试过该数据集，随着层数的增加，能达到99/100的准确率） import tensorflow==2.0.0 Cannot run import tensorflow because of system compatibility. AI Studio prepared an entire environment based on PaddlePaddle already. Please use PaddlePaddle to build your own model or application. ","link":"https://xuaii.github.io/post/svm/"},{"title":"【转载】VR扯掉了中国科技圈的遮羞布","content":"转自知乎 Oculus应该卖给谁？好大的一个问题啊。 卖给谁都可以，但就是不该卖给Facebook！ 卖给Apple多合适，那里有全世界最好的消费电子产品。卖给Google也不错，安卓系统逐渐统治世界，Google眼镜刚被《时代》杂志评为那一年最令人期待的硬件产品。 还可以选微软，VR（Virtual Reality，虚拟现实）最直观的使用场景就是游戏，这和微软Xbox游戏机一脉相承。就连亚马逊，好歹也做出了Kindle，在人类出版界放了一场大火。 再看看Facebook呢？一点做硬件的经验都没有。扎克伯格(Mark Zuckerberg)曾经放出狠话要做手机，折腾了半天，只是竹篮打水一场空。 不仅如此，Oculus还非常不facebook。CTO压根没有Facebook账户，CEO的办公室里赫然挂着一副对Facebook不敬的工艺品，员工把嘲笑Facebook当作工作的日常，FB是一个典型的反面教材：僵化的公司、糟糕的设计、隐私的小偷。 当扎克伯格千里迢迢来拜访Oculus全体员工，一位叫Dycus的员工，当着全公司的面问小扎：很多人（当然，不包括我哈），都觉得Facebook是邪恶的（Facebook is evil），你觉得，这会怎么影响Oculus的形象？ 图：戴着Oculus的扎克伯格 但是扎克伯格却像被施了法术，对Oculus着了魔。 扎克伯格带着Facebook一众高管轮流戴了一会Oculus的VR头盔。 当时还只是样品，距离成品上市，还有很远的距离。即使到了今天，VR头盔都没有克服最大的bug——晕眩。更别说7年前了。小扎卸下头盔的刹那，估计和学生时代在哈佛大学通宵宿醉后一样的难受，但是，小扎连同CTO和负责产品的VP，对这个新奇的玩意赞不绝口，反复说着：holy crap（专业翻译：我X, NB啊）。 5天后，这位全球最大社交网络的管理者亲自去了这家创业公司的办公室。 这一天，这位全世界最富有的80后背了一个麦当劳的购物袋。这真是神来之笔。本来对Facebook没有一点好感的年轻极客们，瞬间议论纷纷：真是没想到，互联网超级大佬，原来是和我一样的人。 在狠狠拉了一波好感值后，扎克伯格为这家创办仅仅2年的公司开出了无法拒绝的价码——30亿美元。以及反复的劝诱和许诺：Oculus会成为Facebook唯一的平台，我要让Oculus成为VR的领导者！ 怎么拉近资本和创业者的距离，小扎可能是全世界最懂这件事的人。当年Facebook刚刚成立，小扎要去红杉资本的办公室谈融资。在肖恩·帕克（Sean Parker）的教唆下，他穿着睡衣走进了高大上的办公室。 西装革履的中年油腻男被挑逗得欲火焚身，这才是TMD真极客！投他，必须投他！ 那只麦当劳的手提袋，很难说不是一场精心设计的轮回。这位哈佛辍学生，有一个木讷的外表，以至于常常让我们忽略了他极其精明和复杂的内心世界。 但是。 搞VR？！你还是想想清楚吧，这个专门绞杀科技巨头的战场，早已白骨累累。 01 从弗兰克鲍姆的《万能钥匙》到赫胥黎的《美丽新世界》，从伍迪艾伦的《沉睡者》（1973年），到斯皮尔伯格的《头号玩家》（2018年），VR（虚拟现实）是所有科幻小说、科幻电影的必备元素。 带上一套装备，通过视觉、嗅觉、触觉，让人走进一个完全虚拟的世界。还有比这更cool的科技产品吗？ 斯皮尔伯格的电影《头号玩家》主题就是一个VR游戏 写出《数字化生存》的MIT（麻省理工）教授尼葛洛庞帝说，他从1969年就开始关注VR。 没错，在连彩色电视机都没有普及的60年代，真极客们就已经在尝试做VR头盔。90年代，任天堂、世嘉，以及乔布斯实习过的雅达利，先后对着VR设备，大搞特搞。 世嘉（SEGA）搞得最轰动。产品宣传了三年，发布会开了又开，《大众科技》的杂志封面也如愿登过了。结果，临门一脚，突然宣布不发布了，放了全世界一个大鸽子。世嘉的理由是：VR游戏太过真实，玩家在游戏过程中不由自主地移动，容易受伤。 拉倒吧。 受伤是真的，但不是因为移动，而是因为晕眩导致的恶心头疼。VR营造了一个深度沉浸的世界，如果技术跟不上，虚拟画面和现实动作有延时，会造成严重的晕眩，类似现实中的晕车。 从此，晕眩感成为横亘在VR产业面前的一道天堑，或者说一道魔咒。人类科技界对VR的第一次攻关，以被团灭的方式告终。 紧接着，互联网来了。亚马逊、Google先后问世，那些利用互联网连接存量资源，那些消费主义的迎合者，成了硅谷的新宠。 时代换了一批新的宾客，但是极客的火苗没有完全熄灭。把科幻小说里最cool的装备变成现实，也是科技新贵的梦。 Google先做AR(Augmented reality，增强现实)，谷歌眼镜（Google Glass）才出了几张宣传照，全世界科技迷就自嗨到了高潮，不仅有英国王子这样的社会名流野生代言，而且频频被各类媒体评价为人类最具创新、最值得期待的产品。Google又感动又兴奋，再接再厉做了一个VR的平台Daydream，满腔热血期待复制一个安卓的神话。 三星，作为当时全球出货量第一的手机品牌，也狠狠表示了一把，弄出个Gear VR。但是，Google眼镜、Daydream，GearVR，全部火不过两年，最终销声匿迹。 目前为止，人类最重要的硬件还是智能手机。Google和三星是这个行业绝对的领导者，在产业链里是一呼百应。那么强的能力，那么厚的背景，在VR行业也不过是碰一鼻子灰，羞羞地找个台阶下。 这就是Facebook收购Oculus的背景。在硅谷，Facebook的标签是侵犯隐私、抄袭模仿、功利主义，这样的扎克伯格可能做出人类最新奇的硬件吗？ 02 2016年，小扎正在为Facebook进入中国努力。顶着大雾霾，毅然决然地在天安门前跑步，是个又红又专的好少年。 那次扎克伯格来北京，目的是参加由国务院发展研究中心主办的中国发展高层论坛。 93岁的基辛格老爷子做了《避免修昔底德陷阱》的演讲，接下来就是扎克伯格和马云的谈话。扎克伯克表示，前两年我收购了Oculus，今年会推出一款VR头盔，今年消费级VR就会爆发，五到十年之后，VR手机会成为市场主流。 马云说，行啊，我会帮你。 其实，不只是姓赵。姓马，也是有鄙视链的。 就拿马克·扎克伯格、马云、马斯克来说。 小扎像个中国人，场面上说话一定圆滑，不轻易得罪人。和马云聊天客客气气的，上来先介绍自己正在学习中文，但是还很糟，所以辛苦马云今天用英语交流，谢谢大家的理解。 你再看看马斯克？那副得瑟劲。同样是来中国参加对话，马斯克一点不给马老师面子，把马氏鸡汤里的逻辑矛盾、事实错误全指了出来。喜欢阿里的人千万别放心上，其实玩硬核科技的马斯克最看不起的人，是马克·扎克伯格。马斯克不带修饰地说过：Facebook垃圾（Facebook sucks），窃取隐私、只会垄断，扎克伯格小朋友对于AI一无所知……然后，以身作则地把自己的Facebook账户注销了。 小扎心里是真苦啊。Facebook是全世界市值前五的互联网公司，说起来有头有脸，但是收入的95%都是广告。扎克伯格气不过的时候也想回怼两句，但是射火箭、卖电动的马斯克恐怕要怼一句更厉害的回来，“你个卖广告的，装什么B”，那年轻的小扎又要气得几天睡不着觉了。 马家人的鄙视链说到底就是一句话：互联网企业压根不是科技公司。 清醒点，好不好，你不过是做了一个网页，因为上线得足够早，早早占了个好坑，很多人用。形成了平台，形成了垄断，独占了数据，仅此而已。巴西男足踢中国男足，即使教练席栓一条狗，也能赢。是的，仅此而已。这怎么能配叫高科技？ 在马斯克眼中，扎克伯格就是这种不配的典型。 这就是扎克伯格坚持做VR的第二个背景。 何止是Google、三星，中国也多的是利用VR吹牛逼的大公司。就在扎克伯格和马云亲切交谈的那一年，阿里推出VR购物 “Buy＋”，目标是利用VR让网购变成逛商场的模样。另一个大手笔是领投了美国很火的AR创业公司Magic Leap，7.94亿美元的融资。 同一年，马化腾也给VR站过台，“VR是一场大洗牌，即将开始。就像移动互联网转型一样，上不了船的人将逐渐落伍”。当年由微信引发的“船票论”再次甚嚣尘上。据说腾讯启动了多款VR游戏的制作。还扬言也像Facebook一样做VR硬件，连原型机都搞出来了。然后，就没有然后了。 VR这件事，科技感爆棚，但是每次一燃，就会被泼下一盆又一盆的冷水。归根结底，有这样几个过不去的坎——晕动症、功能单一、缺乏优质内容、设备臃肿、价格昂贵…… 没过多久，Google、三星、阿里、腾讯都撤了，它们都不过是匆匆的过客，都没有成为推动行业前进的力量。 意外的是扎克伯格却固执地坚持了一下。 为什么？ 往低了说，是守护Facebook的社交边界。扎克伯格这个人，永远在警惕Facebook可能的颠覆者。Facebook的入职手册中，写着这样一句话：如果我们不去创造出能杀死Facebook的产品，有人会去做。 马化腾也表示过，VR/AR很可能是下一代社交，颠覆微信的那一种。 的确，微信毕竟只是一个平面的聊天框。用脚趾头就能想明白，如果VR创建一个虚拟space，渣男撩妹的空间会大很多很多。 Facebook开发的VR社交平台Horizon 往高了说，扎克伯格就是憋了一口气，去TMD马家人鄙视链。我要亲自证明Facebook的科技感——我来做一个新硬件，我来扩展人类的边界，我来打碎虚拟世界和现实世界的这面墙。 你们给我看清楚了。 03 扎克伯格有个枯燥且乏味的爱好——寻访有社交竞争力的创业公司，和创始人一见如故，再开一个你无法say no的条件，接着带你飞一段，然后闹掰，最终交恶。 扎克伯格先后收购Instagram和WhatsApp，它们的创始人斯特罗姆(Kevin Systrom)、克里格(Mike Krieger)、库姆(Jan Koum)都在2018年滚蛋。 Oculus自然不会例外，勒基(Palmer Luckey)只在Facebook待了三年就不愉快地离开。剩下的伊里比（Brendan Iribe）也在2018年打包走人。 VR设备有一项路线之争——做一个连接电脑/手机的硬件，还是做一个独立的设备？ 连接电脑/手机的好处显而易见，电脑/手机性能强，VR设备不需要计算，只需要负责显示，因此重量更轻（更适合戴在头上），成本更低。但是缺点也很明显，永远只能当附庸，本质就是一台显示器，一台戴在头上装X的显示器。 早期的Oculus很惊艳，但是选择的技术路线是做一个连接到电脑的显示器，因为创始人的首要的目标是打游戏要爽。 这群死肥宅怎么可能理解马克·扎克伯格的痛苦和野心？ Facebook麾下的Oculus一定要做独立的设备！怎么可以依附在电脑和手机上？电脑，是微软Windows的天下，手机王国已经被苹果iOS和Google安卓的瓜分完了。你们还想我扎克伯格卑躬屈膝到什么时候？ 这些年Facebook Reality Labs玩命地在屏显技术、眼球追踪、手指追踪、面部和身体追踪技术上烧钱，但是有一个前提。 一定要做独立的设备！ 扎克伯格为Oculus设计的战略是，硬件+操作系统+内容生态+应用商店分发+社交。我说得再简单一点哈，苹果和iPhone在智能手机行业是什么地位，Oculus在VR这个人类新硬件上，就要什么地位。 对，苹果有多NB，我就要多，N，B。 04 2016年，AR游戏精灵宝可梦GO （Pokemon Go）惊艳问世，让VR/AR产业实实在在地火了六个月。 紧接着全球VR/AR产业又跌回无休止的寒冰期，一眼望去看不到头的那种。 法律面前不一定人人平等，但是VR面前却做到了人人平等，不管你是Google三星阿里腾讯这样的巨头，还是爱逞强爱嘴炮的腰部互联网，又或者是灵活敢闯的独角兽，反正统统让你们死。 2016年，乐视说，VR是乐视生态重要的组成部分，然后乐视崩盘。暴风说，暴风会成为中国乃至世界VR行业的领导者，然后暴风势不可挡地成了“小乐视”。Magic Leap，AR行业最神秘的公司，拿了Google和阿里巴巴的投资，这个够骚了吧？但是这些年，管它叫“美国乐视”的人越来越多…… 人类就是可笑，折腾了那么久，那么多人那么多钱那么多会，最终还不如一只叫皮卡丘的黄色老鼠有能量——一款游戏为一个行业充电六个月。 踏踏实实地说，VR最基本的使用场景就是玩游戏，因此，出一款口碑爆表的VR游戏，比一万场口嗨发布会都管用。 看看隔壁的任天堂switch，这小破游戏机也默默无闻好多年，最终凭借《健身环大冒险》和《动物森林》两个爆款游戏，在2020年顺利出圈。 VR人喜欢这个励志故事，心里有个信念VR行业一定也能守得云开见月明。 2020年，《半衰期：爱莉克斯》（Half-Life: Alyx）来了。游戏媒体IGN给了罕见的满分评价，说“这像是一款来自未来的游戏”。 还记得小时候整个网吧都在玩的CS吧，《半衰期：爱莉克斯》和CS同宗同源，由顶级游戏公司Valve开发。不仅有人类游戏界的顶级IP坐镇，而且游戏操作用到了“真的VR”。 在PC和智能手机，游戏再好玩也只是二维的平面。但是VR游戏发生在一个三维的空间。在《半衰期：爱莉克斯》里，有怪物向你扑来，你会像现实生活中一样，很自然地举起椅子，把怪物隔开。 动图封面 《半衰期：爱莉克斯》（Half-Life: Alyx） Valve Index Controller指虎型的设计提供了可识别单个手指动作的功能，这就是VR的魅力，你可以真的用手玩游戏，而不是用鼠标和键盘——现实里手怎么动，游戏里手就怎么动。 如果说2016年的Pokemon Go是AR游戏的代表作，那么2020年的《半衰期：爱莉克斯》就是VR行业的里程碑。结结实实地让各大电商平台的VR头显卖到了缺货。 AR/VR的使命是打穿虚拟和现实之间的那道墙，2020年，这道墙肉眼可见地在变薄。 软件上陆续有了好游戏，VR硬件也有了普遍的进步。体现在一些专业的指标上，6自由度（6Dof）定位追踪、90Hz刷新率，以及更高的分辨率。 还记得那个该死的晕眩感吗？VR头盔让人晕眩，最主要的原因在于计算能力不行，有延迟。你在真实空间移动了，但是虚拟画面没有跟上同步移动，比如延迟了30毫秒，来来回回，就晕了。和晕车一个道理。 得益于专业AI芯片和AI空间定位算法，加上5G带来延时下降，晕眩的解决指日可待。行业一般认为把延时缩短到15毫秒以内，晕眩感就可以消除，而现在已经无限逼近。 VR硬件接连出现好设备，Facebook的Oculus Quest 2和索尼的PSVR，是公认的两强。 索尼有PS游戏机的底子，算是干好了老本行，但是Facebook这个互联网暴发户也杀了进来，确实让人刮目相看。 另外，PSVR不是独立设备，需要连接索尼PS主机才能玩。Oculus Quest一问世，索尼的市场份额节节败退。索尼感觉到了后继乏力，明确表示回家闭关修炼内功，到2022年再发布新产品，一决雌雄。 这样看来，Oculus势不可挡的劲头会延续一会。江湖有了传说，要等武林盟主苹果亲自出手，才能杀一杀Oculus的嚣张气焰。 扎克伯格可算是扬眉吐气了。 05 2014年，腾讯收购WhatsApp的交易几乎已经谈妥，就差马化腾和WhatsApp创始人Jan Koum见一面。 人算不如天算，关键时刻，马化腾腰伤的老毛病又犯了，决定先做手术，于是见面推迟。 不料扎克伯格趁虚而入，给Jan Koum开出190亿美元的天价。当时，WhatsApp才成立五年，50个人。 190亿美元！那还等什么马化腾牛化腾。Deal！ 时隔七年来看，马化腾的一次腰疼，让腾讯丢掉了国际化最好的抓手。时至今日，这家市值7万亿港元的中国第一互联网公司，年收入的95%都在中国，连跨国企业都称不上。你说你是靠竞争赢的，不是靠垄断，不过在全球市场，你好像没赢过吧？ 但是，2014年，谁能看得那么深远？微信支付奇袭支付宝，腾讯正在高歌猛进，革命气氛一片大好。 直到美国的科技媒体the Information写了一篇文章《Facebook应该向微信学习什么》（What Facebook Should Learn from WeChat），中国互联网圈，除了阿里巴巴脸色不好，别的从业者都集体高潮了。 辛辛苦苦这么多年，我们做的都是copy to china。永远都是美国什么火了，就copy一下去中国做一个一样的。中国的创业者去华尔街、硅谷讨钱，BP画得再精致老外也听不懂，远远不如换一个谦卑的口吻，有一种意味深长的声音说，“我们做的这个，就是这是中国的Facebook”，“中国的Yelp”，“中国的Amazon”……老外立刻懂了，百试不爽。 哪能想到有生之年能看到这个？！多豪情啊，是正儿八经的西方媒体，严肃的告诫，Facebook应该向中国的微信学习！甚至有人开始造一个新词“copy from china”。 厉害了，我的中国互联网。 不过，这故事还有一个更骚的结尾。2019年，扎克伯格竟然亲自回应了这篇文章，承认自己错了，说，悔不听君一席话，早学微信就好啦。 很奇怪，这一次，中国的舆论一点都不嗨了。嗨不起来。没错，腾讯和Facebook各自管理着中国最大和世界最大的社交产品，腾讯的市值甚至已经稳稳地超过了Facebook。 但是。 腾讯三分之一的收入是游戏。一年1000多亿元的游戏收入，常年在世界游戏公司收入中排名第一。讽刺的是，这个第一既做不出像使命召唤、星际争霸这样征服全球的3A游戏，也不能像Microsoft、任天堂、索尼一样，做出Xbox、Switch、PS这样的顶级游戏硬件。 这个星球上最赚钱的游戏公司，却对游戏行业的进步提供不了一丝一毫的推动。他是全世界最尴尬的游戏之王。 这就不难理解，在突如其来的VR时代里，能指望他做啥呢？ 06 VR，以及一脉相承的AR，再到现在新词运动里的MR(Mixed Reality，混合现实)和XR（Extended Reality，拓展现实），核心是一样的：把现实和虚拟更深地搞到一起。 纵观人类产业历史，一项高精尖的技术往往经历先军用，再商用/民用的发展过程。除了深度游戏玩家和科技迷，AR/VR距离普通人的生活还有一些远，但是其在军用和2B业务上已经站稳了脚跟，而消费级的民用产品，已经走到了爆发的前夜。 对一个人来说，付出总有回报。对一个产业来说，也是功不唐捐。 VR产业的核心技术包括定位技术、光学元件、操作系统、芯片、传感技术、人工智能、电子织物。VR头盔看似是一台游戏机，但实际上是一场高精尖技术的大阅兵。 比如，VR行业普遍使用的SLAM技术，在实时性和精度上要求非常高。能胜任VR的技术，反手就可以用于自动驾驶和机器人。 再比如，VR中的语音识别也有很高的要求，不仅要读懂语意，还要匹配嘴部运动，不仅要做到实时性，还要能捕捉面部动作。 重中之重是AI技术，VR中的定位和识别，一方面需要硬件支撑，一方面，要靠AI做了几十亿次的模拟。中国的科技大佬，动不动就说自己对于人工智能的投入，如今，是骡子是马，是时候拉出来遛遛了。 为什么说VR扯掉了中国科技商业圈的遮羞布？ 因为可以确定的是，中国产业界一定会重复自己在PC和智能手机时代一模一样的遗憾——造不来核心技术，站不上主导地位。 VR里，芯片还是高通的。如果说智能手机还有华为、苹果两个巨头自研芯片的杂音，那么VR行业就安静很多，Oculus、HTC，国内的小鸟、大朋所有主流设备的芯片都是高通的。 再比如，生态的争夺还是在硅谷内部。微软Windows操作系统了PC时代，智能手机有iOS和安卓争锋，同样，VR生态也会有操作系统。目前来看，Facebook走在前面，后面跟着微软（Windows Mixed Reality）和苹果这两个老家伙的追赶，至此，拿着望眼镜看，操作系统这件事也和中国无关。 内容开发更是中国企业弱项中的弱项，Oculus Quest上线了上百款游戏，只有一两款来自中国。 在VR战场的最前沿，唯一的中国元素应该是歌尔，提供代工，和部分非核心元器件。 VR一定会和PC和智能手机一样，重复一样的剧本——先来的人建好了生态，后来的玩家永远只能当绿叶或者当肥料，像今天一样，继续交苹果税，交安卓税，交高通税…… PC时代、智能手机时代，中国企业还有借口，因为发展浅，因为底子薄，除了联想有钱，都找不出能做大事的大公司。但是这一次，面对VR这个新硬件，没借口了吧，那么多互联网企业在中国挣到了惊掉西方同僚下巴的大利润。 当年，联想提“贸工技”，被骂惨了——你看，联想多短视，赚了钱不好好搞研发，没有卡住关键技术和关键位置，混成今天这样真是活该。 然后，产业界用了20年时间，只为了向世界证明，我们人人都是联想。 去做真的科技公司，在管理好财务平衡的同时，尽最大可能地用科技拓宽人类的边界，而不是只盯着短期能赚钱的事，靠垄断利润，把市值垒得如此虚高——这是中国产业界最要学习，却永远无法吸取的教训。 就拿VR来说，先行者和长期主义者的曙光渐渐展现。 按照销售数据估算，Oculus Quest 1和Oculus Quest 2的累计销量即将突破1000万。1000万什么概念？智能手机轻轻松松一年卖出10亿台，PC不断衰落，但是全球年销量依旧在2-3亿。这样看来1000万（而且还是累计数），简直不值一提，距离小扎自己吹的牛B，“要让10亿人使用Oculus”，还差9.9亿。 但是，行业里的参与者才知道，1000万，意味着行业的生态成型，已经有了规模经济，从此内容生产者可以赚到钱了，会有越来越多的开发者投入进来。内容越丰富，生态就越吸引用户。万事开头难，但Oculus迈过了第一道槛，这个生态将开始良性循环。 扎克伯格真是够心狠手辣的，把Oculus的价格杀到299美元，比一台普通智能手机还便宜。这是什么概念？前面提到Vavle公司自己的VR设备要999美元，吓退一众屌丝，不可能普及。华为绝对是国产之光和价格屠夫，但是华为VR产品的价格只能压到3000元人民币，而且和Oculus比，华为VR第一不是独立设备，第二性能差一截，第三生态差，没有好游戏。 Oculus的299美元，就好比早已是行业第一却在不断降价的特斯拉，后来者想进入这个行业，但是大门正在被扎克伯格一点点关上。 Facebook和他的VR叙事，应该是中国同行的一面镜子。 在VR身上，值得我们羡慕的是美国的商业生态。巨头公司敢打科技硬仗，重金投入在周期长、定位高的赛道，耐心孵化造生态，要干就要干核心技术。 这恰恰是中国科技界、产业界最短缺的。短视、功利，什么赚钱做什么，已经是深入灵魂的信仰。往深了说，因为你们没有极客精神，从来没有把扩宽人类边界当作自己的使命。以为提倡员工穿牛仔裤，给员工配mac电脑就是科技公司了，其实呢，大厂的胸牌再明光晃晃，一开口说话都是市侩的味道，是一个赶集的买卖人，其实很土的。 为什么会这样？ 第一是纵容垄断，而垄断的原因又是市场不开放。比如，说了那么多，其实中国根本买不到Oculus，毕竟Facebook都还没有进入中国市场。拿腾讯来说，只有遇到体量相当的竞争对手，才能走出“没有梦想”的诅咒。 第二，知识产权的保护还是远远不够。在中国只有傻子才好好做游戏和内容，你做得再好，第二天就有了免费下载的破解版。指望“用爱发电”，能完成产业迁跃，笑话。 新硬件之战，或者说，生态之战怎么打？这需要，大公司敢打硬仗，小公司能靠内容本身赚到钱。而我们暂时还没有这样的土壤。 VR真有意思，它的目标是拆掉虚拟和现实之间的那道墙。在我看来，中国要实现VR产业的大突破，我们的目标应该是先拆掉现实生活里的某些墙。 谢谢读完，虚拟人VR创始人倪志力，对此文亦有指导 ","link":"https://xuaii.github.io/post/zhuan-zai-vr-che-diao-liao-zhong-guo-ke-ji-quan-de-zhe-xiu-bu/"},{"title":"游戏机制-里世界","content":"里世界机制 机制概念简述：存在两个不同的世界，里世界和表世界 他们有大致相同的场景整体形状，不同的场景风格（例如，崭新&lt;-&gt;老旧，清洁&lt;-&gt;污染） 有一个透镜能在表世界看见里世界的物体并且互动 只有玩家能在里世界和表世界间穿梭，这意味着玩家可以通过切换世界来躲避伤害 所有的外部物体（除玩家外），与透镜发生碰撞时可以 以一种特效的的方式穿过，暗示对玩家没有伤害 直接与透镜碰撞然后消失 通过对透镜的形状限制来设计关卡 实现：表示世界和里世界将在一个场景节点中表示 透镜物体是一个Light2D， 通过光照Mask显示/剔除里世界的物体的Sprite 透镜物体是一个Area2D，通过记录进入区域中的里世界物体，并且实时修改里世界物体的碰撞形状，如果是表世界（那么根据简述3来处理） public class SuperLens : Area2D { public Dictionary&lt;string, InnerObject&gt; data = new Dictionary&lt;string, InnerObject&gt;(); public void OnObjectExit(Node node) { if(node is InnerObject innerObject &amp;&amp; data.ContainsKey(innerObject.Name)) { data.remove(innerObject.Name); } } public void OnObjectEnter(Node node) { if(node is InnerObject innerObject ) { data[innerObject.Name] = innerObject; } if(node is OuterObject outerObject ) { ProcessOuterObject(outerObject); } } public override void _PhysicsProcess(float delta) { foreach(var inner in data.Values) { // 更改碰撞体积 inner.Rect = inner.DefaultRect.overlap(this.Rect); } } } ","link":"https://xuaii.github.io/post/you-xi-ji-zhi-li-shi-jie/"},{"title":"游戏机制-窗体互动","content":"使用 Godot 对WindowFrame 进行拙劣的模仿，观察视频中的窗体有以下性质： 窗体既是 UI 也是可以与玩家互动的 场景物体 窗口的位置可以由场景物理改变也可以由鼠标控制改变 玩家可以发射子弹，子弹碰到的边进入锁定状态，一段时间后子弹消失，并且解除锁定 考虑一下几种设计： 方案一 存在一个窗体对象window和场景对象rect 每一帧将rect大小和位置经过MPV变换后同步到窗口大小 每一个rect 对象有一个默认的初始大小 rect 对象持有四条边的对象 line，每个line对象有 follow/move/lock/idle状态 Follow 状态的边跟随 target(player) 运动 Move 状态的边能被鼠标拖动 Lock 状态的边在玩家移动过程中充当 墙 或 地板 Idle 状态，是场景中没有 target 时的状态 rect 对象实进入场景树的时候生成 windows 对象，并持有，rect退出场景树的时候回收 windows 对象 各个状态都有对应的 Physics Collision Layer / Mask，例如锁定状态的边会阻挡某些攻击，拖动时和锁定时与玩家的碰撞是不一样的。 using Godot; using System; public class InteractiveAnchor : RigidBody2D { public Vector2 velocity; public bool IsStop = false; public Vector2 StopPosition = Vector2.Zero; ColorRect rect; RandomNumberGenerator _random = new RandomNumberGenerator(); public delegate void InteractiveAnchorCallBack(InteractiveAnchor anchor); public InteractiveAnchorCallBack callback; // cache 一旦锁定目标，不可更改！！ InteractiveLine AnotherCache = null; public override void _Ready(); public void Init(Vector2 _velocity, InteractiveAnchorCallBack _callback); public override void _ExitTree(); // // Called every frame. 'delta' is the elapsed time since the previous frame. public override void _PhysicsProcess(float delta); // 添加抖动将要删除时 public void _on_DeleteEffect_timeout(); void _on_Area2D_body_entered(Node body); void _on_Area2D_body_exited(Node body); void CacheCurrentState(Vector2 origin_global_position); } public class InteractiveArea : Area2D { public override void _Ready(); Node Scene; public override void _EnterTree(); public override void _PhysicsProcess(float delta); void _on_Area2D_body_entered(Node body); void _on_Area2D_body_exited(Node body); } public class InteractiveBorder : Node2D { // 显示区域大小 public Rect2 window; [Export] public Rect2 DefalutRect; [Export] public float BorderWidth; public InteractiveLine left; public InteractiveLine right; public InteractiveLine top; public InteractiveLine down; public InteractiveWindow interactiveWindow; public Vector2 Start; public Vector2 End; public Rect2 UIRect; public override void _Ready(); public override void _EnterTree(); public override void _Draw(); public override void _Process(float delta); public override void _PhysicsProcess(float delta); public void Reset(); public void SetRect(Rect2 _window); public void SetCollision(bool flag); } public class InteractiveLine : KinematicBody2D { public bool IsLocked; static public string IsSlideName = &quot;&quot;; public SegmentShape2D shape; public InteractiveWindow window; public Vector2 velocity = Vector2.Zero; public int ColllisionCount = 0; [Export] public Vector2 DefaultBias; public override void _Ready(); public override void _ExitTree(); public override void _PhysicsProcess(float delta); public void SetCollision(uint layer, uint mask); public void Reset(); public float DistanceToLine(Vector2 P, Vector2 A, Vector2 B); } public class InteractiveLineFollow : StateNode&lt;InteractiveLine&gt; { public override void Enter() { target.SetCollision(target.window.FollowLayer, target.window.FollowMask); } public override void _PhysicsUpdate(float delta) { /// 如果没有目标，就转移到Lock？ if(!target.window.IsLockTarget) { _machine.Transition&lt;InteractiveLineIdle&gt;(); return; } if(target.window.Target.IsInsideTree()) { target.GlobalPosition = target.window.Target.GlobalPosition + target.DefaultBias; } if(target.ColllisionCount != 0) { _machine.Transition&lt;InteractiveLineLock&gt;(); return; } } public class InteractiveLineIdle : StateNode&lt;InteractiveLine&gt; { public override void _PhysicsUpdate(float delta) { if(target.window.IsLockTarget) { _machine.Transition&lt;InteractiveLineFollow&gt;(); return; } } } public class InteractiveLineLock : StateNode&lt;InteractiveLine&gt; { Vector2 cachePosition = Vector2.Zero; public override void Enter() { target.IsLocked = true; cachePosition = target.Position; target.SetCollision(target.window.LockLayer, target.window.LockMask); } public override void Exit() { target.IsLocked = false; } public override void _PhysicsUpdate(float delta) { // TODO:这里是否可以不移动锁定的目标不移动 if(cachePosition != null) target.Position = cachePosition; if(target.ColllisionCount == 0) { _machine.Transition&lt;InteractiveLineFollow&gt;(); return; } if(Input.IsActionJustPressed(&quot;mouse_right&quot;) &amp;&amp; target.NormalToLine(target.GetGlobalMousePosition(), target.GlobalPosition + target.shape.A, target.GlobalPosition + target.shape.B).Length() &lt; 14f) { // 进入拖动状态 _machine.Transition&lt;InteractiveLineMove&gt;(); return; } } } public class InteractiveLineMove : StateNode&lt;InteractiveLine&gt; { public override void Enter() { target.SetCollision(target.window.MoveLayer, target.window.MoveMask); } public float PlayerMargin = 50f; private float MoveSpeed = 50f; public override void _PhysicsUpdate(float delta) { if(!target.window.IsLockTarget) { _machine.Transition&lt;InteractiveLineIdle&gt;(); return; } Vector2 line_movement = target.NormalToLine(target.GetGlobalMousePosition(), target.GlobalPosition + target.shape.A, target.GlobalPosition + target.shape.B); Vector2 player_movement = target.NormalToLine(target.window.Target.GlobalPosition, target.GlobalPosition + target.shape.A, target.GlobalPosition + target.shape.B); if(line_movement.Dot(player_movement) &lt; 0f || !target.window.Target.TestMove(target.window.Target.GlobalTransform, line_movement * delta) || player_movement.Length() &gt; PlayerMargin) { // 鼠标位置与Player在line异侧不限制速度，player与line距离足够大不限制速度；其余情况限制速度 if(line_movement.Dot(player_movement) &lt; 0f || player_movement.Length() &gt; PlayerMargin) { target.GlobalPosition += line_movement; } else { target.GlobalPosition += line_movement.Normalized() * MoveSpeed * delta; } } if(Input.IsActionJustReleased(&quot;mouse_right&quot;)) { _machine.Transition&lt;InteractiveLineLock&gt;(); return; } } } public class InteractiveWindow : Node2D { // window local position [Export] PackedScene bullet; [Export(PropertyHint.Layers2dPhysics)] public uint MoveMask; [Export(PropertyHint.Layers2dPhysics)] public uint MoveLayer; [Export(PropertyHint.Layers2dPhysics)] public uint FollowMask; [Export(PropertyHint.Layers2dPhysics)] public uint FollowLayer; [Export(PropertyHint.Layers2dPhysics)] public uint LockMask; [Export(PropertyHint.Layers2dPhysics)] public uint LockLayer; ShaderMaterial shader; public InteractiveBorder border; Main screen; // canvas + window related WindowDialog dialog; CanvasLayer canvas; // screen effects Tween tween; RandomNumberGenerator _random = new RandomNumberGenerator(); Queue&lt;InteractiveAnchor&gt; queue = new Queue&lt;InteractiveAnchor&gt;(); // mouse related // physics's disabled area Area2D area; RectangleShape2D shape; [Export] Vector2 SoftMargin = new Vector2(5, 5); // target lock related public bool IsLockTarget = false; public KinematicBody2D Target = null; public Console _wrapper; public override void _Ready(); public override void _EnterTree(); public override void _ExitTree(); void _DeferredPrograce() { screen.RemoveChild(canvas); screen.ResetScreen(); this.AddChild(canvas); } public override void _PhysicsProcess(float delta) { // 1. window 更新 + 可视区剪裁 screen.SetDisplayRect(border.Start, border.End); // 2. windowdialog 对齐 dialog.Popup_(border.UIRect.Clip(screen.viewport.GetVisibleRect())); // 3. 物理开启区域对齐 area.Position = border.window.Position - SoftMargin; shape.Extents = border.window.Size + SoftMargin * 2; } public void OnWindowGuiInput(InputEvent inputEvent) { if (inputEvent is InputEventMouseButton mouseEvent &amp;&amp; mouseEvent.Pressed) { switch ((ButtonList)mouseEvent.ButtonIndex) { case ButtonList.Left: { if(IsLockTarget) { InteractiveAnchor anchor = bullet.Instance&lt;InteractiveAnchor&gt;(); anchor.GlobalPosition = GetGlobalTransform().AffineInverse() * GetViewportTransform().AffineInverse() * (border.Start + mouseEvent.Position); this.CallDeferred(&quot;add_child&quot;, anchor); // anchor.CallDeferred(&quot;Init&quot;, border.Start + mouseEvent.Position - BasicFollowCamera.Target.ScreenPosition, (object)EffectCallback); anchor.Init((border.Start + mouseEvent.Position - BasicFollowCamera.Target.ScreenPosition), EffectCallback); } break; } case ButtonList.WheelUp: break; } } if (inputEvent is InputEventKey keyEvent &amp;&amp; keyEvent.Pressed) { switch ((int)keyEvent.Scancode) { case (int)KeyList.L: { break; } } } } // delete callback public void EffectCallback(InteractiveAnchor anchor); void disturb_offset(float _strength) { dialog.RectPosition += new Vector2(_random.RandfRange(-_strength, _strength), _random.RandfRange(-_strength, _strength)); screen.GetNode&lt;TextureRect&gt;(&quot;CanvasLayer/_ScreenTexture&quot;).RectPosition += new Vector2(_random.RandfRange(-_strength, _strength), _random.RandfRange(-_strength, _strength)); } public void LockTarget(KinematicBody2D target); public void UnlockTarget(); public void EnableBackground(); public void DisableBackground(); public void SetRect(Rect2 _window); public void Reset(); // CommandLine private void LockPlayer(); private void _on_ActivateArea_body_entered(Node body); } 总结：这样的设计有以下缺陷 拖动 Line 时不能直接修改 velocity，因为拖动时Line有可能会与Player发生碰撞（需要KinematicBody2D 的SlideAndCollide方法更新速度），所以只能设计一种有鼠标位置计算 Line.velocity 的方法。 窗口Window对象在场景的UI节点下，借此将场景相机的画面绘制到窗口相应的位置，这样设计与物体外的结构耦合。 如果想实现窗口外的一切对象停止更新比较麻烦（仅仅 Update 相机内的对象），应该有引擎自带的方法比较方便。 方案二 不使用 UI 对象，将交互和物理全部集中到一个 Rect 对象中。 交互窗体包含四条边，每条边包含碰撞设置和鼠标靠近检查（锁定状态） 每条边拥有相似的几种状态，特别的在拖动时，依然使用velocity来移动每条边 当鼠标左键按下 &amp;&amp; 鼠标位置在边的拖动区域时 ==&gt; 进入拖动状态 松开鼠标左键时 ==&gt; 退出拖动状态 拖动状态下，鼠标位置，边位置，边速度的关系如下 velocityL=(posM−posL)⋅normalL∣∣normalL∣∣velocity_L = \\frac{(pos_M - pos_L) \\cdot normal_L}{||normal_L||} velocityL​=∣∣normalL​∣∣(posM​−posL​)⋅normalL​​ 这样，窗口移动始终比鼠标位置延后一帧，但是移动过程中可以进行物理检测 3. 窗口根据三条边的位置绘制窗口（场景中绘制），并且生成一个mask覆盖整个屏幕（不使用方案一中生成相机纹理在windows中显示） 4. 如果某条边的移动会使得 player 与其他物体碰撞，那么该移动应该被修正: 会与player碰撞的物体有两种可能： a. 场景中的物体 -&gt; 丢弃当前的移动 b.窗体中的自由对边 -&gt; 保持当前移动 c.窗体中的锁定对边 -&gt; 丢弃当前移动 为了使移动边感知到玩家的碰撞，需要为边增加一个Push状态（不用增加，写在Move里即可），该状态下每次移动时应该对玩家进行移动测试，如果通过测试则移动否则不移动。 5. 在边的Move状态下，应该停止相机对 Player的跟踪 6. 2D 引擎不能如3D一样剔除视锥外的物体，不渲染/不更新窗口外的物体只能通过碰撞来实现 a. 所有进入窗体的对象将恢复更新，退出窗体的对象停止更新 b. 场景对象持有所有场景物体的引用，所以当玩家进入窗口时可以直接调用场景节点的Stop方法，这将停止所有除（player，window，obj_in_window）的对象。 ","link":"https://xuaii.github.io/post/you-xi-ji-zhi-chuang-ti-hu-dong/"},{"title":"状态机的几种写法","content":"在各个Godot初学者教程中，实现第一个人物控制器都是使用的状态机，常见的有以下写法： 写法一 全在 Update 中 using Godot; using System; public class ScriptName : Node { // fields [Export] float Speed； [Export] float JumpHeight; private State state = State.Idle; public enum State : int { Run, Jump, Idle, Fall } public void Update { switch(state) { case State.Idel: // code case State.Run: // code case State.Jump: // code case State.Fall: // code default: // code } } } 优点 ： 至少能实现功能， 缺点： 不利于维护，不方便拓展 写法二 写成状态类和状态机类 public interface IState { void Enter(); void Exit(); void Update(float delta); void PhysicsUpdate(float delta); } public class StateBase&lt;T&gt; : IState { private StateMachine _machine; private T agent; public virtual void Enter() { } public virtual void Exit() { } public virtual void Update(float delta) { } public virtual void PhysicsUpdate(float delta) { } } public class StateMachine : Node { Dictionary&lt;string, IState&gt; StateInfo; IState CurrentState; public void ChangeTo&lt;T&gt;() where T : IState { ... } public override void _Process(float delta){ CurrentState.Update(delta); } public override void _Process(float delta) { CurrentState.PhysicsUpdate(delta); } } class RunState : StateBase&lt;T&gt; { ... } class JumpState : StateBase&lt;T&gt; { ... } class IdleState : StateBase&lt;T&gt; { if(Mathf.Abs(horizontalInput) &gt; 0.01) { _machine.ChangeTo&lt;RunState&gt;(); } if(verticalInput &gt; 0.1) { _machine.ChangeTo&lt;Jump&gt;(); } } 优点:比写法一便于维护 缺点：如果要实现一个 Player 的状态机 和一个Enemy的状态机，他们都有相似的移动逻辑，但是Player状态机多出一些技能相关的状态；如果采用上述写法，ChangeTo()是硬编码在状态内部的，这样就不方便添加边和状态以拓展状态机 写法三 ：基于 GraphEditor的状态机 Godot Asset Store 有很多基于 GraphEdit 的状态机实现，通过： 为状态绑定状态脚本 为转移边绑定脚本或者添加转换条件 减少状态 和 转移边 的耦合。 它们都有一个共同的缺点：不支持 c# 或者c#版本bug多 所以能不能实现一个足够简单的，C#版本的，解耦状态和转化边的状态机呢？ 写法四 状态类 using Godot; using Godot.Collections; namespace StateMachine.Base { public interface IState { string StateName {get;} void Init(Object agent, Dictionary&lt;string, object&gt; blackboard, IStateMachine machine); void OnEnter(); void OnExit(); void Update(float delta); void PhysicsUpdate(float delta); void Exit(); } public class StateBase : Object, IState { // state 只持有 agent 对象不持有状态机对象 protected string _StateName = &quot;&quot;; public string StateName =&gt; _StateName; public Object agent; public IStateMachine machine; private Dictionary&lt;string, object&gt; blackboard; public virtual void Init(Object _agent, Dictionary&lt;string, object&gt; _blackboard, IStateMachine _machine) { agent = _agent; blackboard = _blackboard; machine = _machine; } public virtual void OnEnter() { } public virtual void Update(float delta) { } public virtual void PhysicsUpdate(float delta) { } public virtual void OnExit() { } public virtual void Exit() { machine.Transition(StateName); } } } 状态类内部不能调用StateMachine的ChangeTo()方法，只能实现状态自身的更新逻辑 工具类 IStateMachine 接口，Condition 是用于转换边的方法（返回true就转换，否则就不转换） 满足AssertTo.condition 则转换到AssertTo.nextState Transition的属性用于标志该方法是判断哪个状态转移到哪个状态的 public interface IStateMachine { void Transition(string current); void Start(string name); void Exit(); } public delegate bool Condition(); public class AssertTo : Godot.Object { public Condition condition; public string nextState; public AssertTo(Condition _condition, string _nextState) { condition = _condition; nextState = _nextState; } } [System.AttributeUsage(System.AttributeTargets.Method)] public class Transition : System.Attribute { public string from; public string to; public Transition(string _from, string _to) { from = _from; to = _to; } } 状态机类 状态机类继承自 StateBase，因此状态机是可嵌套的 public class StateMachineBase : StateBase, IStateMachine { public IState CurrentState = null; protected Dictionary&lt;string, Array&lt;AssertTo&gt;&gt; TransitionMap = new Dictionary&lt;string, Array&lt;AssertTo&gt;&gt;(); public Dictionary&lt;string, Godot.Object&gt; Name2State = new Dictionary&lt;string, Godot.Object&gt;(); // 状态机黑板 protected Dictionary&lt;string, object&gt; blackboard; public override void Init(Godot.Object _agent, Dictionary&lt;string, object&gt; _blackboard = null, IStateMachine _machine = null) { base.Init(_agent, _blackboard, _machine); if(blackboard == null) { blackboard = new Dictionary&lt;string, object&gt;(); } if(_machine == null) { } // 初始化状态列表 &amp;&amp; 初始化状态名 foreach(var info in this.GetType().GetFields(BindingFlags.Public | BindingFlags.NonPublic | BindingFlags.Instance)) { if(info.ReflectedType.IsSubclassOf(typeof(StateMachine.Base.StateBase))) { if(Name2State.ContainsKey(info.Name)) { GD.Print(&quot;[FSM runtime] duplicate state &quot;, info.Name); } else { IState state = info.GetValue(this) as IState; if(state == null) { // current state is null before start continue; } Name2State[info.Name] = state as Godot.Object; state.Init(agent, blackboard, this as IStateMachine); (state as Godot.Object).Set(&quot;_StateName&quot;, info.Name); } } } // 初始化转换边列表 foreach(var info in this.GetType().GetMethods(BindingFlags.Public | BindingFlags.NonPublic | BindingFlags.Instance)) { if(info.GetCustomAttribute&lt;Transition&gt;() is Transition transition) { if(!Name2State.ContainsKey(transition.from) || !Name2State.ContainsKey(transition.to)) { return; } if(!TransitionMap.ContainsKey(transition.from)) { TransitionMap[transition.from] = new Array&lt;AssertTo&gt;(); } TransitionMap[transition.from].Add(new AssertTo((Condition)Delegate.CreateDelegate(typeof(Condition), this, info), transition.to)); } } } #region life cycle public override void OnEnter() { Start(&quot;entry&quot;); } public override void Update(float delta) { if(CurrentState == null) return; CurrentState.Update(delta); } public override void PhysicsUpdate(float delta) { if(CurrentState == null) return; CurrentState.PhysicsUpdate(delta); } public override void Exit() { if(machine == null) { CurrentState = null; } else { base.Exit(); } } // API public void Transition(string current) { if(CurrentState == null) return; if(CurrentState is ExitState) { CurrentState.OnExit(); Exit(); return; } if(!Name2State.ContainsKey(current)) { return; } if(TransitionMap == null) { return; } if(!TransitionMap.ContainsKey(current)) { return; } foreach(AssertTo edge in TransitionMap[current]) { if(edge.condition.Invoke()) { // 添加信号 CurrentState.OnExit(); CurrentState = Name2State[edge.nextState] as IState; CurrentState.OnEnter(); return; } } } public void Start(string name) { if(Name2State.ContainsKey(name)) { CurrentState = Name2State[name] as IState; CurrentState.OnEnter(); } } } Demo // 状态定义 public class Idle : StateBase { public override void OnEnter() { PlayAnimation(&quot;Idle&quot;); } public override void PhysicsUpdate(float delta) { if(InputCache != null) { Exit(); } } } public class Run : StateBase { public override void OnEnter() { PlayAnimation(&quot;Run&quot;); } public override void PhysicsUpdate(float delta) { Position += velocity * delta; } } public class PlayerStateMachine : StateMachineBase { // 绑定状态名和状态类型 EntryState entry = new EntryState(); Run run = new Run(); Idle idle = new Idle(); ExitState exit = new ExitState(); // 定义转换 [Transition(&quot;entry&quot;, &quot;idle&quot;)] public bool transition_1() { return true; } [Transition(&quot;idle&quot;, &quot;run&quot;)] public bool transition_2() { if(horizontalInput != 0) { return true; } return false; } [Transition(&quot;run&quot;, &quot;idle&quot;)] public bool transition_3() { if(agent.IsOnGround() &amp;&amp; velocity == agent.GetGroundVelocity()) { return true; } return false; } } 这样写就把状态 和 状态转移 分离开了，另外还可以实现一些编辑器工具来生成状态机的文件，以及为指定节点绑定状态机以简化操作 Todo: 每个状态机绑定的对象类型不固定，但是状态机继承应该与 agent 对象继承具有相似的层级，所以每写一个新的状态机就强制转换agent类型过于繁琐，尝试找到优化方法！ ","link":"https://xuaii.github.io/post/zhuang-tai-ji-de-ji-chong-xie-fa/"},{"title":"学习计划","content":"近期计划 GAMES101 计算机图形学入门 时间 : 22 * 1.5h = 33h 链接 : GAMES101 目的 : 基本概念和原理，基本渲染管线 GAMES102 几何建模与处理 时间 : 1.5 * 15 = 22.5hs 链接 : GAMES102 目的 : 游戏引擎中几何体的表达和计算 GAMES203 三维重建和理解 时间 : 14 * 1h = 14h 链接 : GAMES203 目的 : 基本概念, GAMES202 *高质量实时渲染 时间 : 14 * 2h = 28h 链接 : GAMES202 目的 : shader?? GAMES104 现代游戏引擎 时间 : 20 * 2h = 40h 链接 : GAMES104 目的 : 理解游戏引擎中 现存的/未存 的 功能/hack 的原理，各个模块的功能和为什么存在这种功能。 Todo List TheBook of Shaders 主要是造型函数，噪声等 各个游戏引擎中实现的光照方案🔦🔦 设计模式（c#） Carbon语言🏭 游戏常用系统模块设计（对象池，消息总线，存档系统） 游戏玩法功能设计（杂） 🎮 游戏引擎插件/服务 游戏设计相关（ui/关卡/就请/输入布局/核心能力/设计方法） 辅助工具（可视化状态机，可视化行为树，技能编辑器）💻 美术相关软件（blender，GIMP）（不会学的👍 ","link":"https://xuaii.github.io/post/xue-xi-ji-hua/"},{"title":"Fourier Transform","content":"1. Dirichlet Conditions 一个周期内, 连续或者有有限个第一类间断点 一个周期内, 有限个极值点 一个周期内可积 2. Trangle Transform 假设 f(t)=c0+∑n=1∞cncos⁡(nωt+φ)=c0+∑n=1∞[cncos⁡φcos⁡(nωt)−cnsin⁡φsin⁡(nωt)]f(t) = c_0+\\sum_{n=1}^{\\infty}c_n\\cos(n \\omega t + \\varphi) =c_0+\\sum_{n=1}^{\\infty}[c_n\\cos\\varphi\\cos(n\\omega t)- c_n\\sin\\varphi \\sin(n\\omega t)] f(t)=c0​+n=1∑∞​cn​cos(nωt+φ)=c0​+n=1∑∞​[cn​cosφcos(nωt)−cn​sinφsin(nωt)] 令 an=cncos⁡φ,bn=−cnsin⁡φa_n = c_n \\cos\\varphi,\\qquad b_n = -c_n\\sin\\varphian​=cn​cosφ,bn​=−cn​sinφ f(t)=c0+∑n=1∞[ancos⁡(nωt)+bnsin⁡(nωt)]∫0Tf(t)sin⁡(kωt)dt=∫0Tc0sin⁡(kωt)dt+∫0Tsin⁡(kωt)∑n=1∞[ancos⁡(nωt)+bnsin⁡(nωt)]dt∫0Tf(t)sin⁡(nωt)dt=0+bnT2bn=2T∫0Tf(t)sin⁡(nωt)dt f(t) = c_0+\\sum_{n=1}^{\\infty}[a_n\\cos(n\\omega t) + b_n \\sin(n\\omega t)]\\\\ \\int_0^T f(t)\\sin(k\\omega t) dt= \\int_0^Tc_0\\sin(k\\omega t) dt + \\int_0^T\\sin(k\\omega t)\\sum_{n=1}^{\\infty}[a_n\\cos(n\\omega t) + b_n \\sin(n\\omega t)] dt\\\\ \\int_0^T f(t)\\sin(n\\omega t) dt= 0 + b_n \\frac{T}{2}\\\\ b_n = \\frac{2}{T} \\int_0^T f(t)\\sin(n\\omega t) dt f(t)=c0​+n=1∑∞​[an​cos(nωt)+bn​sin(nωt)]∫0T​f(t)sin(kωt)dt=∫0T​c0​sin(kωt)dt+∫0T​sin(kωt)n=1∑∞​[an​cos(nωt)+bn​sin(nωt)]dt∫0T​f(t)sin(nωt)dt=0+bn​2T​bn​=T2​∫0T​f(t)sin(nωt)dt 与 2 相似的可以计算得到 cn,an,φ,c0c_n, a_n, \\varphi, c_0cn​,an​,φ,c0​ 3. Fourier 级数 根据欧拉公式: ejx=cos⁡x+jsin⁡xcos⁡(nωt)=ejnωt+e−jnωt2sin⁡(nωt)=ejnωt−e−jnωt2je^{jx} = \\cos x + j\\sin x\\\\ \\cos(n\\omega t) = \\frac{e^{jn\\omega t} + e^{-jn\\omega t}}{2}\\\\ \\sin(n\\omega t) = \\frac{e^{jn\\omega t} - e^{-jn\\omega t}}{2j}\\\\ ejx=cosx+jsinxcos(nωt)=2ejnωt+e−jnωt​sin(nωt)=2jejnωt−e−jnωt​ (8) (9)代入(3) f(t)=c0+∑n=1∞[anejnωt+e−jnωt2+bnejnωt−e−jnωt2j] f(t) = c_0+\\sum_{n=1}^{\\infty}[a_n \\frac{e^{jn\\omega t} + e^{-jn\\omega t}}{2} + b_n \\frac{e^{jn\\omega t} - e^{-jn\\omega t}}{2j}]\\\\ f(t)=c0​+n=1∑∞​[an​2ejnωt+e−jnωt​+bn​2jejnωt−e−jnωt​] 由于 : an=2T∫0Tf(t)cos⁡(−nωt)dt=a−n同理:−bn=b−n a_n = \\frac{2}{T} \\int_0^T f(t)\\cos(-n\\omega t)dt = a_{-n}\\\\ 同理:-b_n = b_{-n} an​=T2​∫0T​f(t)cos(−nωt)dt=a−n​同理:−bn​=b−n​ 整理得到: f(t)=c0+∑n=1∞[an−jbn2ejnωt+ a−n−jb−n2e−jnωt]f(t)=c0+∑n=1∞an−jbn2ejnωt+∑n=−∞−1an−jbn2ejnωt合并得到:f(t)=∑n=−∞∞an−jbn2ejnωt令:An=an−jbn2f(t)=∑n=−∞∞Anejnωt f(t) = c_0 + \\sum_{n=1}^{\\infty}[\\frac{a_n-jb_n}{2}e^{jn\\omega t} + \\ \\frac{a_{-n} - jb_{-n}}{2}e^{-jn\\omega t}]\\\\ f(t) = c_0 + \\sum_{n=1}^{\\infty}\\frac{a_n-jb_n}{2}e^{jn\\omega t} + \\sum_{n = -\\infty}^{-1} \\frac{a_n - jb_n}{2}e^{jn\\omega t}\\\\ 合并得到:\\\\ \\qquad\\qquad f(t) = \\sum_{n=-\\infty}^{\\infty}\\frac{a_n - jb_n}{2}e^{jn\\omega t}\\\\ 令:\\\\ \\qquad\\qquad A_n = \\frac{a_n - jb_n}{2}\\\\ f(t) = \\sum_{n=-\\infty}^{\\infty}A_ne^{jn\\omega t}\\\\ f(t)=c0​+n=1∑∞​[2an​−jbn​​ejnωt+ 2a−n​−jb−n​​e−jnωt]f(t)=c0​+n=1∑∞​2an​−jbn​​ejnωt+n=−∞∑−1​2an​−jbn​​ejnωt合并得到:f(t)=n=−∞∑∞​2an​−jbn​​ejnωt令:An​=2an​−jbn​​f(t)=n=−∞∑∞​An​ejnωt 在 4. 中得到 Fourier 级数, 再两边同时 乘以 e−jkωte^{-jk\\omega t}e−jkωt 并在一个周期内积分得到: ∫0Tf(t)e−jnωtdt=∫0T∑n=−∞+∞Anej(n−k)ωtdt∫0Tf(t)e−jnωtdt=AnTAn=1T∫0Tf(t)e−jnωtdt \\int_{0}^{T}f(t)e^{-jn\\omega t}dt = \\int_{0}^{T}\\sum^{+\\infty}_{n = -\\infty}A_ne^{j(n-k)\\omega t}dt\\\\ \\int_0^Tf(t)e^{-jn\\omega t}dt = A_nT\\\\ A_n = \\frac{1}{T}\\int_0^Tf(t)e^{-jn\\omega t}dt ∫0T​f(t)e−jnωtdt=∫0T​n=−∞∑+∞​An​ej(n−k)ωtdt∫0T​f(t)e−jnωtdt=An​TAn​=T1​∫0T​f(t)e−jnωtdt 4. Fourier Transform 1 An=1T∫0Tf(t)e−jnωtdtF(ω)=lim⁡T→∞AnT=∫0∞f(t)e−jnωtdt(FourierTransform+)lim⁡T→∞An=lim⁡T→∞F(ω)T=lim⁡T→∞F(ω)⋅ω2π A_n = \\frac{1}{T}\\int_0^Tf(t)e^{-jn\\omega t}dt\\\\ F(\\omega) = \\lim_{T\\rightarrow \\infty}A_nT = \\int_0^\\infty f(t)e^{-jn\\omega t}dt\\qquad (Fourier Transform + )\\\\ \\lim_{T\\rightarrow \\infty}A_n = \\lim_{T\\rightarrow \\infty}\\frac{F(\\omega)}{T} = \\lim_{T\\rightarrow \\infty}\\frac{F(\\omega)\\cdot \\omega}{2\\pi}\\\\ An​=T1​∫0T​f(t)e−jnωtdtF(ω)=T→∞lim​An​T=∫0∞​f(t)e−jnωtdt(FourierTransform+)T→∞lim​An​=T→∞lim​TF(ω)​=T→∞lim​2πF(ω)⋅ω​ 结合 (19)(25) 得到 f(t)=lim⁡T→∞∑n=−∞∞Anejnωt=lim⁡T→∞∑n=−∞∞F(ω)⋅ejnωt2πdω=12π∫−∞∞F(ω)⋅ejnωtdω(FourierTransform−) f(t) = \\lim_{T\\rightarrow \\infty}\\sum^\\infty_{n=-\\infty}A_n e^{jn\\omega t} =\\lim_{T\\rightarrow \\infty}\\sum^\\infty_{n=-\\infty}\\frac{F(\\omega)\\cdot e^{jn\\omega t}}{2\\pi} d\\omega =\\frac{1}{2\\pi}\\int^\\infty_{-\\infty}F(\\omega)\\cdot e^{jn\\omega t}d\\omega \\qquad (Fourier Transform -)\\\\ f(t)=T→∞lim​n=−∞∑∞​An​ejnωt=T→∞lim​n=−∞∑∞​2πF(ω)⋅ejnωt​dω=2π1​∫−∞∞​F(ω)⋅ejnωtdω(FourierTransform−) 5. Convolution Theorem 定理描述设:f1(t)的Fourier变换为F1(ω),f2(t)的Fourier变换为F2(ω),那么:时域:F[f1(t)⊗f2(t)]=F1(ω)⋅F2(ω)频域:F[f1(t)⋅f2(t)]=12πF1(ω)⊗F2(ω)设:f_1(t) 的Fourier变换为F_1(\\omega), f_2(t) 的Fourier变换为F_2(\\omega), \\\\ 那么:\\\\ 时域:\\\\ \\qquad\\qquad F[f_1(t)\\otimes f_2(t)] = F_1(\\omega)\\cdot F_2(\\omega)\\\\ 频域:\\\\ \\qquad\\qquad F[f_1(t)\\cdot f_2(t)] = \\frac{1}{2\\pi}F_1(\\omega)\\otimes F_2(\\omega)\\\\ 设:f1​(t)的Fourier变换为F1​(ω),f2​(t)的Fourier变换为F2​(ω),那么:时域:F[f1​(t)⊗f2​(t)]=F1​(ω)⋅F2​(ω)频域:F[f1​(t)⋅f2​(t)]=2π1​F1​(ω)⊗F2​(ω) 6. Fourier 时移性质 F[f(t)]=F(ω)则:F[f(t−τ)]=F(ω)e−jnωτF[f(t)] = F(\\omega) \\qquad则: F[f(t - \\tau)] = F(\\omega)e^{-jn\\omega \\tau} F[f(t)]=F(ω)则:F[f(t−τ)]=F(ω)e−jnωτ 证明: F[f(t−τ)]=∫−∞+∞f(t−τ)e−jnωtdtF[f(t - \\tau)] = \\int^{+\\infty}_{-\\infty}f(t-\\tau)e^{-jn\\omega t}dt\\\\ F[f(t−τ)]=∫−∞+∞​f(t−τ)e−jnωtdt 令 x=t−τ:x= t - \\tau:x=t−τ: F[f(t−τ)]=∫−∞+∞f(x)e−jnω(x+τ)dx=ejnωτ∫−∞+∞f(x)e−jnωxdx=F(ω)⋅ejnωτ F[f(t - \\tau)] = \\int^{+\\infty}_{-\\infty}f(x)e^{-jn\\omega (x+\\tau)}dx =e^{jn\\omega\\tau}\\int^{+\\infty}_{-\\infty}f(x)e^{-jn\\omega x}dx = F(\\omega)\\cdot e^{jn\\omega\\tau} F[f(t−τ)]=∫−∞+∞​f(x)e−jnω(x+τ)dx=ejnωτ∫−∞+∞​f(x)e−jnωxdx=F(ω)⋅ejnωτ 7. Convolution Theorem 证明 定义卷积运算 f1(t)⊗f1(t)=∫−∞+∞f1(τ)f2(t−τ)dτf_1(t)\\otimes f_1(t) = \\int_{-\\infty}^{+\\infty}f_1(\\tau)f_2(t - \\tau)d\\tau\\\\ f1​(t)⊗f1​(t)=∫−∞+∞​f1​(τ)f2​(t−τ)dτ 将 (35) 带入(24) F[f1(t)⊗f1(t)]=∫−∞+∞[∫−∞+∞f1(τ)f2(t−τ)dτ]e−jnωtdt=∫−∞+∞f1(τ)[∫−∞+∞f2(t−τ)e−jnωtdt]dτ(调换积分顺序)=∫−∞+∞f1(τ)F2(ω)e−jnωτdτ=F2(ω)⋅F1(ω) F[f_1(t)\\otimes f_1(t)] = \\int_{-\\infty}^{+\\infty}[\\int_{-\\infty}^{+\\infty}f_1(\\tau)f_2(t - \\tau)d\\tau]e^{-jn\\omega t}dt\\\\ = \\int_{-\\infty}^{+\\infty}f_1(\\tau)[\\int_{-\\infty}^{+\\infty}f_2(t - \\tau)e^{-jn\\omega t}dt]d\\tau \\qquad(调换积分顺序)\\\\ = \\int_{-\\infty}^{+\\infty}f_1(\\tau)F_2(\\omega)e^{-jn\\omega \\tau}d\\tau\\\\ = F_2(\\omega)\\cdot F_1(\\omega)\\\\ F[f1​(t)⊗f1​(t)]=∫−∞+∞​[∫−∞+∞​f1​(τ)f2​(t−τ)dτ]e−jnωtdt=∫−∞+∞​f1​(τ)[∫−∞+∞​f2​(t−τ)e−jnωtdt]dτ(调换积分顺序)=∫−∞+∞​f1​(τ)F2​(ω)e−jnωτdτ=F2​(ω)⋅F1​(ω) ","link":"https://xuaii.github.io/post/fourier-transform/"}]}